{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Cgny4E8V8NGU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai==1.3.9 python-dotenv==1.0.0 PyMuPDF==1.24.2 PyMuPDFb==1.24.1 tqdm tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tiktoken\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"OPENAI_API_TYPE\")\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"OPENAI_API_VERSION\")\n",
        "\n",
        "model_name = os.getenv(\"AZURE_OPEN_AI_MODEL\")\n",
        "\n",
        "token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "H6Z8_eO38qlS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CallOpenAI(user,system):\n",
        "  response = openai.chat.completions.create(\n",
        "              model= model_name, # model = \"deployment_name\".\n",
        "              temperature= 0,\n",
        "              top_p= 0,\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": system},\n",
        "                  {\"role\": \"user\", \"content\": user}\n",
        "              ]\n",
        "          )\n",
        "  return response"
      ],
      "metadata": {
        "id": "UXJBE_y19pCq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets take a contract and try to analyse it without much instruction"
      ],
      "metadata": {
        "id": "u2YvMRDm-Fx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(pdf_path):\n",
        "  pdf = fitz.open(pdf_path)\n",
        "  text = ''\n",
        "\n",
        "  for page in pdf:\n",
        "    text += page.get_text()\n",
        "\n",
        "  num_tokens = len(token_encoding.encode(text))\n",
        "  print(\"Number of tokens in the entire Document: \", num_tokens)\n",
        "  return text"
      ],
      "metadata": {
        "id": "sI9uVaob_7Ie"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_document = extract_text(\"/content/AWS1.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-vel8SEHS_C",
        "outputId": "56c09f3e-fe26-45cb-f7f6-81cc4e98e57a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the entire Document:  11590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"What is the governing courts for Amazon Web Services South Africa ProprietaryLimited\"\n",
        "\n",
        "full_prompt_SD = short_document +\"\\n\\n\" +Question"
      ],
      "metadata": {
        "id": "PJlC7zoDHmpV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = CallOpenAI(full_prompt_SD,\"You are a Professional lawyer who can analyse documents thorougly\")"
      ],
      "metadata": {
        "id": "097gI1cFKWHu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZjJRGvhK_Ra",
        "outputId": "f551d490-215e-473a-8db4-191ae8835e6e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The governing courts for Amazon Web Services South Africa Proprietary Limited would be the South Gauteng High Court in Johannesburg, South Africa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now lets load up a document that has more than 16000 tokens, which is the limit of GPT-3.5-Turbo"
      ],
      "metadata": {
        "id": "B_rAndtLKzTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_document = extract_text(\"/content/PROFRAC HOLDINGS, LLC credit agreement.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja6FHWxeAiKo",
        "outputId": "837a9686-f4e3-4206-c3c4-23d92d60e6c0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the entire Document:  163227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question = \"What does Covered Entity mean in the Document given above?\"\n",
        "\n",
        "full_prompt_LD = long_document +\"\\n\\n\" +Question"
      ],
      "metadata": {
        "id": "dfpmAVqmKCXM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here what you see is, when the message length exceeded the limit of GPT, it throws an error.\n",
        "### This problem will be fixed in the next lab where you see how Retrieval Augmented Generation(RAG) will fix this problem and enable us to analyse documents of any length."
      ],
      "metadata": {
        "id": "ve7bc33tLygQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  response = CallOpenAI(full_prompt_LD,\"You are a Professional lawyer who can analyse documents thorougly\")\n",
        "except Exception as e:\n",
        "  print(str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBWHfMohKxKw",
        "outputId": "8d35ea6d-8955-422a-e31d-ffbf4392b375"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16384 tokens. However, your messages resulted in 163261 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4DAlOKoELobh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}