{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19gcKQZJYhNv"
      },
      "source": [
        "# Streamlining Contract Analysis Using LORA 🤖📄  \n",
        "\n",
        "We are developing an legal assistant designed to provide precise, context-aware answers to legal queries. Instead of manually reviewing lengthy contracts, users can simply ask questions, and extract and summarize relevant information from structured legal datasets. Whether it’s identifying key clauses, summarizing obligations, or clarifying terms, our solution enhances efficiency and accuracy in legal document analysis empowering users to make informed decisions with ease.  \n",
        "\n",
        "## Training Methodology 📚  \n",
        "To ensure high-quality responses, our AI is trained using:  \n",
        "\n",
        "- A structured dataset containing a wide range of Master Service Agreement (MSA) examples and corresponding questions.  \n",
        "- A testing dataset to evaluate and refine the model’s accuracy.  \n",
        "- Each training example includes:  \n",
        "  - **Document content**, systematically organized by page numbers.  \n",
        "  - **Relevant legal questions**.  \n",
        "  - **Verified correct answers** for precise responses.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KunXwqqgWPIz"
      },
      "source": [
        "\n",
        "## Let's Break It Down the Solution ! 📦\n",
        "\n",
        "### 1. `transformers`\n",
        "- It contains pre-trained AI models (like having a smart student ready to learn more)\n",
        "- Think of it as the brain of our AI system\n",
        "\n",
        "### 2. Helper Tools\n",
        "- `accelerate`: Makes our AI training faster (like a turbo boost!)\n",
        "- `pyboxen`: Helps make our output look nice and organized\n",
        "- `datasets`: Helps us organize and handle our training data\n",
        "- `peft`: This is our special LoRA teaching tool (version 0.4.0)\n",
        "\n",
        "### 3. Update PEFT\n",
        "- Makes sure we have the newest version of our special teaching method\n",
        "- PEFT (Parameter Efficient Fine-Tuning) is what makes our training efficient\n",
        "\n",
        "## Why Do We Need These? 🤔\n",
        "Imagine building a house:\n",
        "- `transformers` is like having the main structure\n",
        "- `accelerate` is like having power tools instead of hand tools\n",
        "- `datasets` is like having an organized toolbox\n",
        "- `peft` is like having special techniques to build faster and better\n",
        "\n",
        "## Important Note 📝\n",
        "We need to run this cell first because:\n",
        "1. It sets up all our necessary tools\n",
        "2. Without these, none of our later code will work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQFkHz7LNiLl",
        "outputId": "daacf52d-01b3-4c9c-9619-6a2a562702b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.50.2-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.0\n",
            "    Uninstalling transformers-4.50.0:\n",
            "      Successfully uninstalled transformers-4.50.0\n",
            "Successfully installed transformers-4.50.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.4.0)\n",
            "Collecting peft\n",
            "  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.50.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2023.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n",
            "Downloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.4.0\n",
            "    Uninstalling peft-0.4.0:\n",
            "      Successfully uninstalled peft-0.4.0\n",
            "Successfully installed peft-0.15.1\n"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade transformers\n",
        "! pip install -q accelerate pyboxen datasets==2.17.0 peft==0.4.0 pyboxen\n",
        "! pip install --upgrade peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRpL_y6BWPI3"
      },
      "source": [
        "\n",
        "# Cell 2: Importing Our Packages 🧰\n",
        "\n",
        "Before we start working, we need to import all the packages. Think of this like taking out all your tools from the toolbox and arranging them on your workbench!\n",
        "\n",
        "### 1. Basic Tools 🔨\n",
        "- `os`: Like having a map of your computer files\n",
        "- `torch`: The main engine that powers our AI (like a car's engine)\n",
        "- `datetime`: Our clock to track when things happen\n",
        "- `warnings`: Helps keep our workspace tidy by hiding unnecessary messages\n",
        "- `requests`: Like a messenger that can fetch things from the internet\n",
        "\n",
        "### 2. AI Tools 🤖\n",
        "From `transformers` we get:\n",
        "- `AutoModelForCausalLM`: Our base AI model (like getting a smart student)\n",
        "- `AutoTokenizer`: Translates human words into AI language\n",
        "- `TrainingArguments`: Rules for how to teach our AI\n",
        "- `pipeline`: Makes using AI easier (like having preset recipes)\n",
        "- `logging`: Keeps track of what our AI is doing\n",
        "\n",
        "### 3. LoRA Teaching Tools 📚\n",
        "From `peft` we get:\n",
        "- Tools to make our AI learn efficiently\n",
        "- Special ways to update only parts of our AI\n",
        "- Settings to control how our AI learns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "on5VPPoA0IZK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, PeftModel, LoraConfig, get_peft_model\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsO-UTMqWPI3"
      },
      "source": [
        "# Cell 3 Understanding Our Dataset📚\n",
        "\n",
        "## What is Our Dataset?\n",
        "Our dataset is a collection of Master Service Agreements (MSAs) formatted in a special way to teach our model how to understand and answer questions about legal documents.\n",
        "\n",
        "## How is Our Dataset Formatted? 🏗️\n",
        "Our dataset is stored in JSONL format (JSON Lines), where each line contains:\n",
        "\n",
        "1. **Document Content**\n",
        "   ```json\n",
        "   {\n",
        "     \"page_number\": \"1\",\n",
        "     \"content\": \"Page 1 of 8 EXPEL, INC. USER AGREEMENT TERMS AND CONDITIONS...\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "2. **Question-Answer Pairs**\n",
        "   ```json\n",
        "   {\n",
        "     \"question\": \"Does this contract mention what happens to customer data after termination?\",\n",
        "     \"answer\": \"Yes, the contract specifies that within 30 days after termination...\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "3. **System Instructions**\n",
        "   ```json\n",
        "   {\n",
        "     \"role\": \"system\",\n",
        "   \"content\": \"You are a seasoned lawyer with a strong background in Master Service Agreement...\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "## What Information Does Our Dataset Contain? 📄\n",
        "1. **Legal Documents**\n",
        "   - Complete MSA texts\n",
        "   - Page numbers and content organization\n",
        "   - Terms and conditions\n",
        "   - Legal clauses and provisions\n",
        "\n",
        "2. **Questions**\n",
        "   - Data handling queries\n",
        "   - Termination clauses\n",
        "   - Contract duration\n",
        "   - Legal obligations\n",
        "   - Customer rights\n",
        "\n",
        "3. **Answers**\n",
        "   - Detailed explanations\n",
        "   - References to specific sections\n",
        "   - Legal interpretations\n",
        "   - Clear, simple responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vSCPI2Fi0Nne"
      },
      "outputs": [],
      "source": [
        "# URLs for accessing the dataset files from GitHub repository\n",
        "test_url = \"https://raw.githubusercontent.com/initmahesh/MLAI-community-labs/main/Class-Labs/Lab-8(Fine-tuning-PEFT-LoRA)/formatted_test_set.jsonl\"\n",
        "train_url = \"https://raw.githubusercontent.com/initmahesh/MLAI-community-labs/main/Class-Labs/Lab-8(Fine-tuning-PEFT-LoRA)/formatted_train_set.jsonl\"\n",
        "\n",
        "# Define local file paths where we'll save our downloaded data\n",
        "test_file_path = \"/content/formatted_test_set.jsonl\"\n",
        "train_file_path = \"/content/formatted_train_set.jsonl\"\n",
        "\n",
        "# Download and save the test dataset\n",
        "response_test = requests.get(test_url)\n",
        "with open(test_file_path, \"wb\") as f_test:\n",
        "    f_test.write(response_test.content)\n",
        "\n",
        "# Download and save the training dataset\n",
        "response_train = requests.get(train_url)\n",
        "with open(train_file_path, \"wb\") as f_train:\n",
        "    f_train.write(response_train.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujSUArBxWPI4"
      },
      "source": [
        "# **Understanding How We Prepare Training Data 📚**  \n",
        "\n",
        "We are transforming large legal documents, such as contracts, into a structured dataset. By breaking them into smaller, well-organized sections, we enable our AI to efficiently process, understand, and retrieve relevant information. This structured approach allows the model to provide accurate answers to user queries based on the dataset context.  \n",
        "\n",
        "## **Why Is This Important? 🎯**  \n",
        "\n",
        "### **1. Structured Organization 📋**  \n",
        "- Ensures legal content is systematically categorized for efficient processing  \n",
        "- Improves accessibility and retrieval of relevant information  \n",
        "- Provides a clear, organized format for model training  \n",
        "\n",
        "### **2. Consistent Learning Framework 📝**  \n",
        "- Establishes a standardized data structure for seamless model learning  \n",
        "- Enables the model to identify patterns and relationships within legal texts  \n",
        "- Enhances accuracy and efficiency in document analysis   \n",
        "\n",
        "## **How It Works: Step by Step ⚙️**  \n",
        "\n",
        "#### **1. Loading the Document 📖**  \n",
        "#### **2. Extracting and Structuring Content 📑**  \n",
        "#### **3. Formatting Data for AI Training 🗂️**  \n",
        "#### **4. Saving the Processed Data 💾**  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB29daK-0hx-"
      },
      "outputs": [],
      "source": [
        "def prepare_training_data(train_file_path, test_file_path):\n",
        "    \"\"\"\n",
        "    Prepare training data specifically for MSA analysis with page_number and content format\n",
        "    \"\"\"\n",
        "    import json\n",
        "\n",
        "    with open(train_file_path, 'r') as f:\n",
        "        train_data = [json.loads(line) for line in f]\n",
        "\n",
        "    formatted_train_data = []\n",
        "    for item in train_data:\n",
        "        # Extract the page content and combine if needed\n",
        "        contents = []\n",
        "        if isinstance(item.get('content'), list):\n",
        "            for page in item['content']:\n",
        "                if isinstance(page, dict) and 'content' in page:\n",
        "                    contents.append(f\"Page {page.get('page_number', '')}: {page['content']}\")\n",
        "        else:\n",
        "            contents.append(item.get('content', ''))\n",
        "\n",
        "        # Create a structured prompt\n",
        "        prompt = f\"\"\"\n",
        "Context: Master Service Agreement content:\n",
        "{' '.join(contents)}\n",
        "\n",
        "Question: {item.get('question', '')}\n",
        "\n",
        "Answer: {item.get('answer', '')}\n",
        "\"\"\"\n",
        "        formatted_train_data.append({\n",
        "            \"text\": prompt.strip()\n",
        "        })\n",
        "\n",
        "    # Save formatted training data\n",
        "    with open('formatted_train.jsonl', 'w') as f:\n",
        "        for item in formatted_train_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "    return 'formatted_train.jsonl'\n",
        "\n",
        "# Call the function\n",
        "formatted_train_path = prepare_training_data(train_file_path, test_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBOqbQlDaEXg"
      },
      "source": [
        "# Cell 5 Loading Our Model: Microsoft Phi-1.5 🤖\n",
        "\n",
        "## What is Phi-1.5?\n",
        "Microsoft Phi-1.5 is like a smart student who's already learned a lot about language and writing. It's a smaller, more efficient AI model that's perfect for learning new specific tasks - like understanding legal documents in our case!\n",
        "\n",
        "\n",
        "## What's Happening Here? 🤔\n",
        "\n",
        "### 1. Choosing Our AI Helper\n",
        "```python\n",
        "model_name = \"microsoft/phi-1_5\"\n",
        "```\n",
        "- Phi-1.5 is known for being good at understanding text\n",
        "- It's smaller and faster than many other AI models\n",
        "\n",
        "### 2. Loading the Model\n",
        "```python\n",
        "model = AutoModelForCausalLM.from_pretrained(...)\n",
        "```\n",
        "- `torch_dtype=torch.float32`: Makes calculations more precise\n",
        "- `low_cpu_mem_usage=True`: Uses less computer memory\n",
        "- `trust_remote_code=True`: Allows the model to use its special features\n",
        "\n",
        "### 3. Setting Up the Translator\n",
        "```python\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "```\n",
        "- Like teaching our AI how to read and write\n",
        "- Helps convert human words into AI language and back\n",
        "- Makes sure the AI understands our questions\n",
        "\n",
        "## Why These Settings Matter? 🎯\n",
        "\n",
        "### 1. Efficiency 🚀\n",
        "- Uses computer resources wisely\n",
        "- Runs faster without needing super powerful computers\n",
        "- Perfect for learning new tasks\n",
        "\n",
        "### 2. Accuracy ✨\n",
        "- Makes precise calculations\n",
        "- Understands text better\n",
        "- Gives more reliable answers\n",
        "\n",
        "### 3. Compatibility 🤝\n",
        "- Works well with our training method\n",
        "- Can handle different types of questions\n",
        "- Easy to teach new things\n",
        "\n",
        "## What Will This Model Do? 📋\n",
        "\n",
        "1. **Read Documents** 📚\n",
        "   - Understands legal language\n",
        "   - Processes long documents\n",
        "   - Remembers important details\n",
        "\n",
        "2. **Answer Questions** ❓\n",
        "   - Finds relevant information\n",
        "   - Explains complex terms\n",
        "   - Gives clear answers\n",
        "\n",
        "3. **Learn and Improve** 📈\n",
        "   - Gets better with training\n",
        "   - Adapts to specific needs\n",
        "   - Becomes more accurate\n",
        "\n",
        "## Think of It Like... 🎓\n",
        "Imagine having a super-smart study buddy who:\n",
        "- Already knows a lot about language\n",
        "- Is ready to learn more about legal documents\n",
        "- Can help explain complicated things in simple words\n",
        "- Gets better at helping the more they practice!\n",
        "\n",
        "Remember: Just like a student needs the right books and tools to learn, our AI needs the right setup to work well! 🌟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRrigShQRnDr"
      },
      "outputs": [],
      "source": [
        "# Load the model and tokenizer\n",
        "model_name = \"microsoft/phi-1_5\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh_1yBvfaXW_"
      },
      "source": [
        "# Cell 6 Testing Our AI's Initial Performance 🔍\n",
        "\n",
        "We're testing how well our model answers questions before any special training. Think of it like giving a student a test before teaching them the subject!\n",
        "\n",
        "\n",
        "## Let's Break It Down! 🤔\n",
        "\n",
        "### 1. The Question We're Asking\n",
        "```python\n",
        "question = \"Does this contract has a provision that mentions that customer data will be deleted upon request?\"\n",
        "```\n",
        "- Like asking a student a specific question about a textbook\n",
        "\n",
        "### 2. Finding Relevant Information\n",
        "```python\n",
        "relevant_parts = [\n",
        "    part['content'] for part in questions\n",
        "    if 'Governing Law' in part['content'] or 'governed by' in part['content'].lower()\n",
        "]\n",
        "```\n",
        "- Like helping the student find the right chapter in the textbook\n",
        "- Currently only looking at parts about \"Governing Law\" (which isn't enough!)\n",
        "\n",
        "### 3. Creating the Question Format\n",
        "```python\n",
        "prompt = f\"\"\"\n",
        "As a legal expert, analyze the following excerpts from a Master Service Agreement:\n",
        "{' '.join(relevant_parts)}\n",
        "{question}\n",
        "Provide a concise answer.\n",
        "\"\"\"\n",
        "```\n",
        "- Like writing the question in a way the student can understand\n",
        "- Includes context and clear instructions\n",
        "\n",
        "## Why Aren't We Getting Good Answers Yet? 🤷‍♂️\n",
        "\n",
        "### 1. Limited Knowledge 📚\n",
        "- The model hasn't been trained on legal documents yet.\n",
        "\n",
        "### 2. Wrong Focus 🎯\n",
        "- We're only looking at \"Governing Law\" sections\n",
        "- Like looking in the wrong chapter of the textbook\n",
        "\n",
        "### 3. Raw Responses 📝\n",
        "- The model gives generic or incorrect answers\n",
        "- Like a student guessing answers without studying\n",
        "\n",
        "## Example of Current Response\n",
        "```\n",
        "Out-of-the-box model response:\n",
        "----------------------------------\n",
        "Based on the provided excerpts, I cannot determine if there is a specific provision\n",
        "about customer data deletion. The given content only discusses governing law aspects.\n",
        "```\n",
        "\n",
        "## What We Need to Fix 🛠️\n",
        "\n",
        "### 1. Better Training\n",
        "- Need to teach the model about legal documents.\n",
        "\n",
        "### 2. Improved Search\n",
        "- Look at all relevant contract sections\n",
        "- Like checking all chapters that might have the answer\n",
        "\n",
        "### 3. Accurate Responses\n",
        "- Train the AI to give precise, factual answers\n",
        "- Like teaching a student to answer based on facts, not guesses\n",
        "\n",
        "## Coming Up Next! 🚀\n",
        "In the next steps, we'll:\n",
        "1. Train the model properly\n",
        "2. Look at all relevant contract sections\n",
        "3. Get more accurate and helpful answers\n",
        "\n",
        "Remember: Just like a student needs proper training to give correct answers, our model needs special training to understand and answer questions about legal documents accurately! 🌟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YOnDmEzrMnj7",
        "outputId": "1d4803cf-0cd7-426c-a837-c8a7b0a896d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "            Results Before Fine-Tuning            \n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                                   \n",
            "   \u001b[33m╭─\u001b[0m\u001b[33m Question \u001b[0m\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m   \n",
            "   \u001b[33m│\u001b[0m                                                                                                           \u001b[33m│\u001b[0m   \n",
            "   \u001b[33m│\u001b[0m   Does this contract has a provision that mentions that customer data will be deleted upon request by     \u001b[33m│\u001b[0m   \n",
            "   \u001b[33m│\u001b[0m   customer or after closure or termination of agreement for whatsoever reason?                            \u001b[33m│\u001b[0m   \n",
            "   \u001b[33m│\u001b[0m                                                                                                           \u001b[33m│\u001b[0m   \n",
            "   \u001b[33m╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m   \n",
            "                                                                                                                   \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                                   \n",
            "   \u001b[34m╭─\u001b[0m\u001b[34m Out-of-the-box model response \u001b[0m\u001b[34m──────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m                                                                                                           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m                                                                                                           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   As a legal expert, analyze the following excerpts from a Master Service Agreement:                      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m                                                                                                           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   You are a seasoned lawyer with a strong background in Master Service Agreement agreement.\\              \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m       Your expertise is required to analyze a Master Service Agreement agreement and answer a question    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   based on that Master Service Agreement agreement.                                                       \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m       The Master Service Agreement agreementagreement is provided in JSON format where each object two    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   keys 'page_number' and 'content', 'page_number' key contains the page number of the page of the         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Master Service Agreement agreementagreement and 'content' key contains the content on that page.        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m                                                                                                           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m                                                                                                           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m       The Master Service Agreement agreement is mentioned below in triple quotes.    Master Service       \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Agreement agreement: '''{'page_number': 7, 'content': 'IN WITNESS WHEREOF, the parties to this          \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Agreement acknowledge they have read this Agreement and understand and agree to be bound by its terms   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   and conditions and hereby execute it through their duly authorized representatives. CUSTOMER ROBIN      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   SYSTEMS, INC. By: Xyz Company By: Name: James doe Name: Anirban (Oni) Chakravartti Title: Finance       \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Head Title: SVP, Global Sales Date: 14 April 2022 Date:'}{'page_number': 2, 'content': 'Customer. An    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   “Affiliate” means any entity under the control of Customer where “control” means ownership of or the    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   right to control greater than 50% of the voting securities of such entity. Customer represents and      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   warrants that it: (i) has the authority to negotiate this Agreement on behalf of each of its            \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Affiliates which will utilize the Robin Products hereunder, and to bind such Affiliates to terms and    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   conditions of this Agreement, and (ii) will be'}{'page_number': 6, 'content': 'the trademarks of        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Customer as a result of this Agreement or otherwise. (d) Severability. If any provision of this         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Agreement is held by a court of competent jurisdiction to be contrary to law, the remaining             \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   provisions of this Agreement will remain in full force and effect. (e) Governing Law. This Agreement    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   and any disputes arising out of, or related to, this Agreement, its termination or the relationship     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   of the parties will be governed by and construed in accordance with the laws of the'}{'page_number':    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   6, 'content': 'confirmation sheet, or by overnight express mail to the parties at their addresses set   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   forth below or to such other address as either party may so designate in writing at least ten (10)      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   days prior to such notice or communication. If to Robin: If to Customer: ROBIN SYSTEMS, INC.            \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   _______________ 2001 Gateway Place, Suite 340 _______________ San Jose, CA 95110 _______________        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Attn: Legal _______________ 18. General. (a) Assignments and Binding Effect. Either                     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   party'}{'page_number': 3, 'content': 'information regarding Customer products or services. Robin may    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   want to incorporate this Feedback into its Robin Products and this clause provides Robin with the       \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   necessary license to do so. Customer hereby grant to us and our assigns a royalty-free, worldwide,      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   perpetual, irrevocable, fully transferable and sublicensable right and license to use, disclose,        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   reproduce, modify, create derivative works from, distribute, display and otherwise distribute           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   and'}{'page_number': 3, 'content': \"for seizure and injunctive relief. If Customer fails to take        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   these steps in a timely and adequate manner, Robin may take them in its own or Customer’s name and at   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Customer’s expense. (b) Customer Responsibilities.  Customer (i) shall use the Robin Products in        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   compliance with all applicable local, state, national and foreign laws, treaties and regulations in     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   connection with Customer's use of the\"}{'page_number': 6, 'content': 'any reasonable guidelines as      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Customer may adopt from time to time. Upon the termination of this Agreement such right shall cease,    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   and Robin shall destroy all advertising, sales literature and other promotional materials containing    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   trademarks of Customer. Robin hereby acknowledges that Customer is the sole and exclusive holder of     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   all right, title, and interest in and to the trademarks of Customer and Robin does not have and will    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   not acquire any interest in or to the trademarks of Customer as'}{'page_number': 5, 'content': 'and     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   foreign government licenses, and Customer shall defend, indemnify and hold Robin and its suppliers      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   harmless from any claims arising out of Customer’s violation of such export control laws. By            \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   accepting this Agreement, Customer confirms that Customer is not a resident or citizen of any country   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   currently embargoed by the U.S.A. list of embargoed countries is available at the official web site     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   of the Office of Foreign Assets Control of the U.S.'}{'page_number': 2, 'content': \"Products to build   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   an application or product that is competitive with any Robin product; (vii) interfere with the proper   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   working of the Robin Products; or (viii) bypass any measures Robin may use to prevent or restrict       \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   access to the Robin Products (or other accounts, computer systems or networks connected to Robin).      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Customer is responsible for all of Customer's activity in connection with the Robin Products,           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   including but not limited to handling Customer's data used with the Robin Products.\"}{'page_number':    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   3, 'content': '5. RESERVED 6. Responsibilities of Customer (a) Unauthorized Use or Disclosure.          \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Customer acknowledges that any unauthorized use or disclosure of the Robin Products may cause           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   irreparable damage to Robin. If an unauthorized use or disclosure occurs, Customer will promptly        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   notify Robin and take, at Customer’s expense, all steps which are necessary to recover the Robin        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Products and to prevent its subsequent unauthorized use or dissemination, including availing itself     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   of actions'}{'page_number': 1, 'content': 'ROBIN SYSTEMS, INC. Master Services Agreement This Master    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Services Agreement (this “Agreement”) is entered into as of the 15 April, 2022 (the “Effective          \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Date”), as defined below, by and between ROBIN SYSTEMS, INC., a Delaware corporation (“Robin”)  and     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   XYZ company, with its principal offices located at New York, U.S.A (“Customer”). RECITALS A. Robin is   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   a developer of virtualization technologies deployed via software products which                         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   enable'}{'page_number': 2, 'content': '(e) “Order Form” means one or more ordering documents that       \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   contain business terms related to Customer’s subscription to use the Robin Products executed by         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Customer and Google through the Google Marketplace and referencing this Agreement. (e) “Product         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Updates” means new versions, updates, bug fixes and enhancements of the Robin Products licensed         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   hereunder and the applicable Order Form that Robin makes available for download, in its sole            \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   discretion, from its website during the License Term.'}{'page_number': 6, 'content': '. Either party    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   may not sell, transfer, or assign this Agreement without the prior written consent of the other         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   party, except in connection with a merger, acquisition, corporate reorganization, or sale of all or     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   substantially all of such party’s assets. Any act in derogation of the foregoing shall be null and      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   void, and the Customer will remain obligated under this Agreement. This Agreement shall benefit         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   and'}{'page_number': 3, 'content': 'exploit any Feedback as we see fit, entirely without obligation     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   or restriction of any kind, except that Robin will not identify Customer as the provider of such        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Feedback. 8. Term and Termination (a) Term. Unless terminated earlier as set forth in this Agreement,   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   the term of this Agreement shall begin on the Effective Date and shall continue until the expiration    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   of all Order Forms.  The term of each Order'}{'page_number': 5, 'content': 'limited to the number of    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Cores, quantity of storage, and at Robin’s request Customer shall certify the foregoing in writing.     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Upon reasonable notice and during regular business hours, Robin shall have the right to audit the use   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   of the Robin Products by Customer (and its affiliates and subsidiaries, if applicable) to verify        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   compliance with this Agreement. If an audit reveals any use of the Robin Products that is out of        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   compliance with this Agreement,'}{'page_number': 3, 'content': 'No rights or licenses are granted       \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   except as expressly and unambiguously set forth in this Agreement. (b) Feedback. Customer may from      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   time to time provide suggestions, comments, information or other feedback with respect to the Robin     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Products (“Feedback”). For the avoidance of doubt, Feedback will only refer to suggestions, comments    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   or other feedback provided to Robin regarding the Robin Products and will not                           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   include'}{'page_number': 6, 'content': 'be binding upon the parties to this Agreement and their         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   respective permitted successors and assigns. (b) No Implied Waiver. The waiver or failure of either     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   party to exercise in any respect any right provided for in this Agreement shall not be deemed a         \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   waiver of any further right under this Agreement. (c) Use of Trademarks. Robin may, in its              \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   advertising, sales literature and other promotional materials, use the trademarks of Customer,          \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   subject to compliance with any reasonable guidelines as'}{'page_number': 2, 'content': '2. RESERVED     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   3. License (a) License Grant. Subject to the terms and conditions of this Agreement, Robin grants       \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   Customer a limited, non-exclusive, non-transferable (except as set forth in Section 17(a)),             \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   non-sublicensable (except as set forth in Section 17(a)) right, during the License Term, solely for     \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   the internal business purposes of Customer, to: (i) reproduce and install the Robin Products on no      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   more than the Maximum Cores; (ii) reproduce copies of the Robin Products for backup purposes;           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   and'}{'page_number': 6, 'content': '2. LICENSE (b) License Terms. The terms and conditions of this      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   License are set forth in the following section of this Agreement. The terms and conditions of this      \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   License shall apply to all Licensees of this Agreement, unless otherwise specified.'}{'page_number':    \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   1, 'content': '(vi) Terms and Conditions. The terms and conditions of the License are as set forth in   \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m   the following section of this Agreement.'}{'page                                                        \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m│\u001b[0m                                                                                                           \u001b[34m│\u001b[0m   \n",
            "   \u001b[34m╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m   \n",
            "                                                                                                                   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyboxen import boxen\n",
        "\n",
        "def generate_response(prompt, model, tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=100,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Load test questions\n",
        "import json\n",
        "with open(test_file_path, \"r\") as f:\n",
        "    questions = [json.loads(line) for line in f]\n",
        "\n",
        "# Test question\n",
        "question = \"Does this contract has a provision that mentions that customer data will be deleted upon request by customer or after closure or termination of agreement for whatsoever reason?\"\n",
        "\n",
        "# Find relevant parts\n",
        "relevant_parts = [\n",
        "    part['content'] for part in questions\n",
        "    if 'Governing Law' in part['content'] or 'governed by' in part['content'].lower()\n",
        "]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "As a legal expert, analyze the following excerpts from a Master Service Agreement:\n",
        "\n",
        "{' '.join(relevant_parts)}\n",
        "\n",
        "{question}\n",
        "\n",
        "Provide a concise answer.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Results Before Fine-Tuning\".center(50))\n",
        "print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Print question in boxen\n",
        "print(boxen(\n",
        "    question,\n",
        "    title=\"Question\",\n",
        "    padding=1,\n",
        "    margin=1,\n",
        "    color=\"yellow\"\n",
        "))\n",
        "\n",
        "# Generate and print response in boxen\n",
        "response = generate_response(prompt, model, tokenizer)\n",
        "print(boxen(\n",
        "    response,\n",
        "    title=\"Out-of-the-box model response\",\n",
        "    padding=1,\n",
        "    margin=1,\n",
        "    color=\"blue\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PB6ek9EbgU2"
      },
      "source": [
        "## Cell 7 What is LoRA\n",
        "\n",
        "\n",
        "Large Language Models (LLMs) are powerful tools for processing and understanding language, but fine-tuning them for specific tasks can be challenging because of their enormous size and computational demands. This is where Low-Rank Adaptation (LoRA) comes in, offering an efficient solution for fine-tuning LLMs without needing to adjust every parameter.\n",
        "\n",
        "Instead of modifying the entire model, LoRA focuses on a small, manageable subset of parameters. Here’s a simplified breakdown of how it works:\n",
        "\n",
        "1. Normally, LLMs use a large matrix of parameters (W0) to make decisions. This matrix is huge and computationally expensive to adjust.\n",
        "\n",
        "2. LoRA introduces two smaller matrices, A and B, which are much narrower than W0. These matrices represent a low-rank update to the model.\n",
        "\n",
        "3. Instead of retraining the entire matrix W0, LoRA modifies only these smaller matrices, making the fine-tuning process much faster and more efficient. The result is a model update that’s nearly as effective as full fine-tuning but requires significantly fewer computational resources.\n",
        "\n",
        "4. In a typical LLM layer, the output is calculated as output = W0x + b0. LoRA adds a new term, BAx, where A and B are the smaller matrices. This allows the model to adapt to new tasks without modifying the original large matrix W0.\n",
        "\n",
        "![Image Description 2](https://drive.google.com/uc?export=view&id=1XnPMJzKwHun6SGkoUgxDtAzozcIxRRTA)\n",
        "\n",
        "\n",
        "*Source: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)*\n",
        "\n",
        "\n",
        "# Hyper-Parameters for LoraConfig\n",
        "\n",
        "## r (Rank):\n",
        "\n",
        "This defines the rank of the low-rank decomposition matrices A and B. A higher rank means more parameters to fine-tune and potentially better performance, but at the cost of increased memory and compute.\n",
        "Value: 16 is moderate rank value balancing efficiency and expressiveness.\n",
        "\n",
        "## lora_alpha:\n",
        "\n",
        "This is a scaling factor applied to the updates from the low-rank matrices A and B before adding them to the original weight matrix W0. It controls how much influence the LoRA layers have over the original model.\n",
        "Value: 32, gives moderate influence to the LoRA updates.\n",
        "\n",
        "## target_modules:\n",
        "\n",
        "These are the specific layers in the model where LoRA is applied. Only these layers will be fine-tuned with LoRA. Examples here include:\n",
        "1. \"o_proj\": The output projection layer.\n",
        "2. \"qkv_proj\": The query, key, and value projections in the transformer.\n",
        "3. \"gate_up_proj\", \"up_proj\", \"down_proj\", \"lm_head\".\n",
        "\n",
        "## bias:\n",
        "\n",
        "Determines whether LoRA will also adjust the bias terms in the model. In this case, \"none\" indicates that the bias terms are not fine-tuned, meaning only weights are updated.\n",
        "\n",
        "\n",
        "## lora_dropout:\n",
        "\n",
        "The dropout rate applied to LoRA layers during training. Dropout helps regularize the model by randomly ignoring some updates during training, reducing overfitting.\n",
        "Value: 0.05 (5% dropout), meaning that 5% of the connections are dropped during fine-tuning.\n",
        "\n",
        "## task_type:\n",
        "\n",
        "The task type that the model is being fine-tuned for. In this case, \"CAUSAL_LM\" means the task is Causal Language Modeling, where the model is predicting the next word or token in a sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nswiGR9A0x3m",
        "outputId": "f3f509b9-2324-4c06-f845-29a675335247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing model for LoRA fine-tuning...\n"
          ]
        }
      ],
      "source": [
        "# 1. First prepare the model\n",
        "print(\"Preparing model for LoRA fine-tuning...\")\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8r-iVOQWPI7"
      },
      "source": [
        "# Cell 8 Configuring LoRA for Our AI Model 🎯\n",
        "\n",
        "\n",
        "## Let's Break Down Each Setting! 📋\n",
        "\n",
        "### 1. LoRA Attention (r=32) 📚\n",
        "- Like deciding how many new things to learn at once\n",
        "- Higher number = can learn more complex things\n",
        "- Lower number = learns faster but simpler things\n",
        "- 32 is like a \"just right\" amount!\n",
        "\n",
        "### 2. Alpha Scaling (lora_alpha=64) 🎚️\n",
        "- Like setting how much attention to pay to new information\n",
        "- Higher number = pays more attention to new learning\n",
        "- Lower number = sticks more to what it already knows\n",
        "- 64 means it will focus well on learning about legal documents\n",
        "\n",
        "### 3. Target Modules 🎯\n",
        "```python\n",
        "target_modules=[\n",
        "    \"o_proj\",        # Output processing\n",
        "    \"qkv_proj\",      # Understanding context\n",
        "    \"gate_up_proj\",  # Decision making\n",
        "    \"up_proj\",       # Learning new patterns\n",
        "    \"down_proj\",     # Simplifying information\n",
        "    \"lm_head\",       # Language understanding\n",
        "]\n",
        "```\n",
        "- Like choosing which subjects to study\n",
        "- Each module handles different types of learning\n",
        "\n",
        "### 4. Dropout (lora_dropout=0.1) 🎲\n",
        "- Like taking short breaks while studying\n",
        "- Helps prevent memorizing without understanding\n",
        "- 0.1 means 10% chance of taking a \"break\"\n",
        "- Helps the model to learn more robustly\n",
        "\n",
        "### 5. Task Type (task_type=\"CAUSAL_LM\") 📖\n",
        "- Tells the model what kind of learning to do\n",
        "- \"CAUSAL_LM\" means learning to predict what comes next\n",
        "- Like learning to complete sentences in a story\n",
        "\n",
        "## Why These Settings Are Important? 🌟\n",
        "\n",
        "### 1. Efficient Learning 🚀\n",
        "- Only updates what's needed\n",
        "- Saves computer memory\n",
        "- Learns faster than traditional methods\n",
        "\n",
        "### 2. Better Understanding 🧠\n",
        "- Focuses on important parts of legal language\n",
        "- Maintains general language knowledge\n",
        "- Adds new legal expertise\n",
        "\n",
        "### 3. Reliable Results ✅\n",
        "- Prevents forgetting old knowledge\n",
        "- Balances new and existing information\n",
        "- Gives consistent answers\n",
        "\n",
        "\n",
        "Remember: Just like how students need the right study plan to learn effectively, our model needs the right LoRA settings to learn about legal documents properly! 🎓"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvcsTSKs0z_g",
        "outputId": "153703f3-99d7-404b-b4bf-e18c54cfb48c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA...\n"
          ]
        }
      ],
      "source": [
        "# 2. Configure LoRA\n",
        "print(\"Configuring LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    # r: Rank dimension - controls how much new information the model can learn\n",
        "    # Higher r = more capacity but slower training\n",
        "    # Lower r = faster training but might miss complex patterns\n",
        "    r=32,\n",
        "    # lora_alpha: Scaling factor for LoRA updates\n",
        "    # Higher alpha = stronger influence of new learning\n",
        "    # Lower alpha = more conservative learning\n",
        "    lora_alpha=64,\n",
        "    # target_modules: Which parts of the model to update\n",
        "    # These are the key components where we want the model to learn\n",
        "    target_modules=[\n",
        "        \"o_proj\", # Output projection layer - final processing\n",
        "        \"qkv_proj\", # Query/Key/Value projections - attention mechanism\n",
        "        \"gate_up_proj\", # Gating mechanism - controls information flow\n",
        "        \"up_proj\", # Upward projection - pattern recognition\n",
        "        \"down_proj\", # Downward projection - information compression\n",
        "        \"lm_head\", #Language model head - final word predictions\n",
        "    ],\n",
        "    # bias: Whether to train bias terms\n",
        "    # \"none\" means we don't update bias terms, focusing only on weights\n",
        "    bias=\"none\",\n",
        "    # 0.1 = 10% dropout rate\n",
        "    lora_dropout=0.1,    # Dropout probability\n",
        "    # CAUSAL_LM means predicting next words based on previous ones\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1zECpde1cfn",
        "outputId": "9c9a39ee-55b2-431d-9b35-16fd101072b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying LoRA to model...\n"
          ]
        }
      ],
      "source": [
        "# 3. Get PEFT model\n",
        "print(\"Applying LoRA to model...\")\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz2mDHbWPI8"
      },
      "source": [
        "# Cell 9 Setting Up Training Arguments for Our AI 🎓\n",
        "\n",
        "## What Are Training Arguments? 🤔\n",
        "Think of training arguments like setting up rules for how our model should study. Just like a student needs a study schedule, breaks, and ways to check their progress!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zy_dk541fUx",
        "outputId": "15e9e0e0-e14d-4edc-89c4-d251f454ea36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,703,936 || all params: 1,419,923,456 || trainable%: 0.12%\n"
          ]
        }
      ],
      "source": [
        "# 4. Print trainable parameters info\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params:,d} || \"\n",
        "        f\"all params: {all_param:,d} || \"\n",
        "        f\"trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a_L9hA8nSQ-"
      },
      "source": [
        "# **Teaching AI to Read Legal Documents 📚**  \n",
        "\n",
        "This process structures legal documents so the AI can interpret them effectively—similar to organizing study notes for better understanding.  \n",
        "\n",
        "## **How It Works 📝**  \n",
        "\n",
        "### **1. Establishing Context**  \n",
        "- Defines a clear framework for interpreting legal content  \n",
        "- Ensures the model analyzes documents with a structured approach  \n",
        "\n",
        "### **2. Retrieving and Organizing Data**  \n",
        "- Searches for existing structured answers  \n",
        "- If none are found, extracts key details from the document, including relevant questions and answers  \n",
        "\n",
        "### **3. Structuring and Formatting Content**  \n",
        "- Organizes information systematically for better readability  \n",
        "- Categorizes and segments legal terms and clauses logically  \n",
        "\n",
        "### **4. Optimizing for Model Processing**  \n",
        "- Converts legal text into a structured, machine-readable format  \n",
        "- Keeps content clear and concise, avoiding unnecessary complexity  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiJj5XMe1kWq"
      },
      "outputs": [],
      "source": [
        "def generate_and_tokenize_prompt(example):\n",
        "    \"\"\"\n",
        "    Modified tokenization for MSA-specific format\n",
        "    Handles both train and validation dataset schemas\n",
        "    \"\"\"\n",
        "    # Format the prompt to include system context for legal analysis\n",
        "    # Tells AI to think like a lawyer Sets the right mindset for legal analysis\n",
        "    system_context = \"You are a legal expert analyzing a Master Service Agreement.\"\n",
        "\n",
        "    # Access 'text' if available, otherwise construct from 'question' and 'answer'\n",
        "    # Gets or creates study material Organizes questions and answers\n",
        "    text = example.get('text', '')  # Get 'text' if present, otherwise empty string\n",
        "    if not text:  # If 'text' is empty\n",
        "        text = f\"Question: {example.get('question', '')}\\nAnswer: {example.get('answer', '')}\"\n",
        "\n",
        "    prompt = f\"{system_context}\\n\\n{text}{tokenizer.eos_token}\"\n",
        "    # Translates into AI language Makes sure content isn't too long\n",
        "    encoded = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    encoded[\"attention_mask\"] = [1] * len(encoded[\"input_ids\"])\n",
        "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
        "\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKIxbrT0WPI9"
      },
      "source": [
        "# Cell 11 Dataset Preparation and Tokenization\n",
        "\n",
        "This cell handles two essential tasks for preparing data for machine learning:\n",
        "\n",
        "#### 1. **Loading Datasets**\n",
        "- The **training dataset** is loaded from a JSON file located at `formatted_train_path` using the `load_dataset` function.\n",
        "- The **validation dataset** is similarly loaded from a file specified by `test_file_path`.\n",
        "- Both datasets are initialized with the `split='train'` parameter, which ensures the entire data is treated as the training split.\n",
        "\n",
        "#### 2. **Tokenizing Datasets**\n",
        "- The `generate_and_tokenize_prompt` function is applied to each data entry in the datasets using the `map` method.\n",
        "- During this process, all original columns (`train_dataset.column_names` and `validation_dataset.column_names`) are removed, keeping only the tokenized results.\n",
        "- The output is two processed datasets, `tokenized_train_dataset` and `tokenized_validation_dataset`, ready for model training.\n",
        "\n",
        "This step ensures the datasets are cleaned, tokenized, and formatted correctly for further processing in the machine learning pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "1a7fe2bf0a4c4cb6acc42239776688af",
            "9e8a567fc0624040bbc72b6d0c05b75d",
            "a7df026e95934cada14e7eeec7b15a6b",
            "00290924d3be460dbaf91e5c4c072994",
            "ffa82fea6cdf4fb5bbd6c8b776640803",
            "f96bfbdeae5e46e5afc8c55519d4ca7a",
            "033f8bb42cd941b3b506c9b99708063f",
            "56eee08c9ea444bc83f48804cdc4a0a8",
            "a083f5a0451240858ff544ad2a95be4f",
            "65f56ce32d0140c78493adc5c1293e4d",
            "c4570e4d13b04016958017d887d8c5a6",
            "662d416033524da980bc95206d690e85",
            "a5fe469264504cc4b2720d2fdf9ad98b",
            "fc3c63846ff94af4b60003584f0fcb3e",
            "daf0900d07744c1593c8c55bfad4ebdc",
            "044f1e76e8904092946e570384e5633d",
            "f0cb37d544134807ac2409feb96fedb6",
            "15e5552e73ed48c0a9c72ed047070e2c",
            "7c2a7d2d0b2e4d30b22e2a33e94bb072",
            "902ffdac27b3434193d92b8bb73a2629",
            "4d3b2f1dcbcf42bcb32eb6b68931b0dd",
            "a89de040f8bb499c888684e763f11f26",
            "2535b492a8c745bfbc622f70ec284d3d",
            "6c63da6c028c4f3db274f15b839871f0",
            "56d137f3df664100b27a0154f192b538",
            "2b3949d47397473998bd38ca77885525",
            "e1233c69531846d3ac32228cef7eeebf",
            "220c6892b6a947a4becc11731daca8ee",
            "cb0fa511c147449a9575a0c090bed367",
            "ceddbb5e420c4c7fa725f45333a1ae5b",
            "7640b0a8a0d14eab985008eb2d4f266d",
            "74aea686b547400e9a6848e25a226145",
            "68d20c88101f4c9f92a50d806d2be448",
            "d1cc447a26144b52a86bca568f24c75e",
            "595272d9de3147b7a592867490dd7bad",
            "fa8eda38af9b4a0f9433dec1e1d75285",
            "32126fbd6c48409c8826de7f61dfc5fc",
            "a341246c41f24983a02232bbdfd7a539",
            "849e0b8faca04dd0826c7c4ce5dc65bd",
            "7b91b58e029343c09dff929d9cec9431",
            "5b0ba0b7d1d8447994c8bb876bdc074b",
            "b3c17ede140c45d9a471595d45fe761f",
            "7985b363e15f41868bcb74de0706c1a4",
            "24961c75465a4f3394a28b7707fd78f0"
          ]
        },
        "id": "S63EzHOS12rD",
        "outputId": "d6e65cb2-f91a-490e-c596-50d5c8d9dc8e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a7fe2bf0a4c4cb6acc42239776688af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "662d416033524da980bc95206d690e85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2535b492a8c745bfbc622f70ec284d3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1cc447a26144b52a86bca568f24c75e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load datasets\n",
        "train_dataset = load_dataset('json', data_files=formatted_train_path, split='train')\n",
        "validation_dataset = load_dataset('json', data_files=test_file_path, split='train')\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train_dataset = train_dataset.map(\n",
        "    generate_and_tokenize_prompt,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "tokenized_validation_dataset = validation_dataset.map(\n",
        "    generate_and_tokenize_prompt,\n",
        "    remove_columns=validation_dataset.column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcCKWucbbq81"
      },
      "source": [
        "# Setting Up the Trainer: Key Notes\n",
        "\n",
        "\n",
        "   This step sets up a **Trainer** using the Hugging Face library to train a machine learning model efficiently with the necessary configurations and datasets.\n",
        "\n",
        "2. **Main Components:**\n",
        "   - **Model:** The pre-trained `model` to be fine-tuned on the given dataset.\n",
        "   - **Datasets:**  \n",
        "     - `tokenized_train_dataset`: The data used for training the model.  \n",
        "     - `tokenized_validation_dataset`: The data used to evaluate the model's performance during training.\n",
        "\n",
        "3. **Training Arguments:**  \n",
        "   These define how the training process will run:  \n",
        "   - **Output Directory:** Saves the model checkpoints and logs in `./phi-1_5-finetune-msa`.  \n",
        "   - **Epochs:** The model will train for 1 epoch (1 full pass through the training dataset).  \n",
        "   - **Warmup Ratio:** 10% of training steps used for learning rate warm-up.  \n",
        "   - **Batch Size:** Processes 4 samples per device during training and evaluation.  \n",
        "   - **Gradient Accumulation:** Accumulates gradients over 4 steps to reduce memory load.  \n",
        "   - **Gradient Checkpointing:** Saves memory during training.  \n",
        "   - **Max Steps:** Limits training to 100 steps.  \n",
        "   - **Learning Rate:** The learning rate is set to `5e-4`.  \n",
        "   - **FP16:** Enables mixed precision for faster training.  \n",
        "   - **Logging:** Logs training details every 10 steps in `./logs`.  \n",
        "   - **Save Strategy:** Saves model checkpoints every 50 steps.  \n",
        "   - **Evaluation:** Evaluates the model every 50 steps and keeps the best model based on **loss**.\n",
        "\n",
        "4. **Data Collator:**  \n",
        "   Uses the `DataCollatorForLanguageModeling` to prepare batches without masked language modeling (`mlm=False`).\n",
        "\n",
        "5. **Training the Model:**  \n",
        "   - Finally, the `trainer.train()` command starts the training process using the configurations and datasets.\n",
        "\n",
        "\n",
        "\n",
        "![Image Description](https://drive.google.com/uc?export=view&id=1evbDx1GhJy907b1BEs5SbMcLSXl7hiYE)\n",
        "\n",
        "*Source: [Guide to Fine-Tuning LLMs with LoRA and QLoRA](https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora)*\n",
        "\n",
        "# This cell sets up a complete framework for model fine-tuning with checkpoints, evaluation, and performance logging integrated and it will take some time to train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "iaVRxYxe2McK",
        "outputId": "8bf99124-b98d-433c-9f22-a3e9fdce53d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 07:40, Epoch 25/34]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>6.692000</td>\n",
              "      <td>1.452453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.397000</td>\n",
              "      <td>1.027894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=6.544816589355468, metrics={'train_runtime': 466.7162, 'train_samples_per_second': 3.428, 'train_steps_per_second': 0.214, 'total_flos': 5150849944780800.0, 'train_loss': 6.544816589355468, 'epoch': 25.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set up the trainer\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "# Disable wandb integration by setting report_to to \"none\"\n",
        "output_dir = \"./phi-1_5-finetune-msa\"\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_validation_dataset,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=1,\n",
        "        warmup_ratio=0.1,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=100,\n",
        "        learning_rate=5e-4,\n",
        "        fp16=True,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"loss\",\n",
        "        greater_is_better=False,\n",
        "        report_to=\"none\"  # Disable wandb integration\n",
        "    ),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V04Zs0lVWPI-"
      },
      "source": [
        "# **Cell 12: Saving the LoRA Model 💾**  \n",
        "\n",
        "This step finalizes the machine learning pipeline by saving the **LoRA (Low-Rank Adaptation) model**.  \n",
        "\n",
        "## **What This Step Does 🛠️**  \n",
        "\n",
        "### **1. Displaying Progress**  \n",
        "- Prints **\"Saving LoRA model...\"** to indicate the process has started.  \n",
        "\n",
        "### **2. Merging and Unloading Weights**  \n",
        "- Calls `model.merge_and_unload()` to **combine** the LoRA adapter weights with the base model and **free up memory**.  \n",
        "\n",
        "### **3. Saving the Merged Model**  \n",
        "- Ensures the target directory (`./phi-1_5-lora-msa-final`) **exists** using `os.makedirs(exist_ok=True)`.  \n",
        "- Saves the **complete model**, including the merged LoRA weights, using `model.save_pretrained()`.  \n",
        "\n",
        "This guarantees that both the **merged model** and the **adapter weights** are securely stored for future use or deployment. 🚀  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDno2ho8-r75",
        "outputId": "2f8f5e1f-0096-40ac-a1cf-463a1f2b0d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving LoRA model...\n"
          ]
        }
      ],
      "source": [
        "# 8. Save the LoRA model\n",
        "print(\"Saving LoRA model...\")\n",
        "\n",
        "# Merge and unload the LoRA weights before saving\n",
        "model.merge_and_unload()\n",
        "\n",
        "# Save the merged model with adapter weights\n",
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(\"./phi-1_5-lora-msa-final\", exist_ok=True)\n",
        "model.save_pretrained(\"./phi-1_5-lora-msa-final\")\n",
        "\n",
        "# Manually save the adapter weights\n",
        "torch.save(model.state_dict(), \"./phi-1_5-lora-msa-final/adapter_model.bin\") # Save adapter weights to adapter_model.bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S87Be2hbWPI-"
      },
      "source": [
        "# **Cell 13: Loading the LoRA Model for Inference 🚀**  \n",
        "\n",
        "This function loads a **pre-trained LoRA model** so it can be used to make predictions.  \n",
        "\n",
        "## **Step-by-Step Process 🛠️**  \n",
        "\n",
        "### **1. Load LoRA Configuration**  \n",
        "- Gets the LoRA settings from the saved model using `LoraConfig.from_pretrained(lora_path)`.  \n",
        "\n",
        "### **2. Set Up the LoRA Model**  \n",
        "- Applies the saved settings to the base model using `get_peft_model()`.  \n",
        "\n",
        "### **3. Load Adapter Weights**  \n",
        "- Loads extra model weights from `adapter_model.bin` using PyTorch’s `torch.load()`.  \n",
        "- First loads the weights on the **CPU** to avoid memory issues on the GPU.  \n",
        "- Uses `strict=False` in `load_state_dict()` to ignore extra details that are not needed.  \n",
        "\n",
        "### **4. Move to GPU (if available)**  \n",
        "- If a **GPU** is available, moves the model to it using `to(torch.device('cuda'))`.  \n",
        "\n",
        "### **5. Return the Model**  \n",
        "- Returns the loaded LoRA model, ready to answer questions.  \n",
        "\n",
        "This function makes sure the **LoRA model is loaded correctly and runs smoothly** on the available hardware. 🚀  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blGJvys0-y9L"
      },
      "outputs": [],
      "source": [
        "# 9. For inference, load the LoRA model properly:\n",
        "def load_lora_model(base_model, lora_path):\n",
        "    # Load the LoRA config and model\n",
        "    config = LoraConfig.from_pretrained(lora_path)\n",
        "    lora_model = get_peft_model(base_model, config)\n",
        "    # Load adapter weights from adapter_model.bin, but only LoRA-related weights\n",
        "    # Load weights to CPU first to avoid CUDA OOM\n",
        "    adapter_weights = torch.load(f\"{lora_path}/adapter_model.bin\", map_location=torch.device('cpu'))\n",
        "    lora_model.load_state_dict(adapter_weights, strict=False) # strict=False ignores missing keys\n",
        "    # Move the model to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        lora_model.to(torch.device('cuda'))\n",
        "    return lora_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu1FpRrSm2DZ"
      },
      "source": [
        "# Cell 14 Compare Out-of-the-Box and Fine-Tuned Model Responses\n",
        "\n",
        "\n",
        "We’ve asked same question our language model in the past, and while the response was relevant, it often felt generic, outdated, or complex. To address this, we leverage the power of **fine-tuned LoRA (Low-Rank Adaptation) models**. By tailoring the model specifically to the task or domain, the generated responses become not only precise but also easier to understand.\n",
        "\n",
        "Now, let’s see how we test and compare both the **base model** and the fine-tuned **LoRA model** in this process.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Dependencies and Setup**\n",
        "- We import libraries for pipeline-based text generation, styled response display, and memory management.\n",
        "- By clearing memory using `gc.collect()` and `torch.cuda.empty_cache()`, we ensure the system is ready for smooth inference.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Generating Responses**\n",
        "- **Base Model:**  \n",
        "  - We test the base model by generating a response for a sample prompt.\n",
        "  - This prompt asks whether a contract contains provisions for customer data deletion.\n",
        "  - While the base model provides a response, it might be vague or too general because it lacks domain-specific tuning.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Testing the LoRA Model**\n",
        "- After the base model, we test the fine-tuned LoRA model:\n",
        "  - The LoRA model is loaded using `load_lora_model()` by merging pre-trained adapter weights from `adapter_model.bin`.\n",
        "  - This model has been fine-tuned for contract-related queries, ensuring its responses are both accurate and relevant.\n",
        "- We generate a response for the same prompt and compare it with the base model's output.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Comparing Results**\n",
        "- The LoRA model response is displayed in a styled green box using the `boxen` library to highlight its improved quality.\n",
        "- The result is noticeably better:\n",
        "  - **Clarity:** The LoRA response is straightforward and easier to understand.\n",
        "  - **Relevance:** It directly addresses the specifics of the question based on fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "Through this process, we demonstrate the impact of fine-tuning. By refining the model for specific tasks, the generated responses are no longer generic—they are **tailored, precise, and user-friendly**, transforming user interactions into something meaningful and efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Oz3lWGvi-3wO",
        "outputId": "b3d42023-0864-4093-c64e-54363730e310"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "                     Testing LoRA Model                     \n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                                   \n",
            "   \u001b[32m╭─\u001b[0m\u001b[32m LoRA Model Response \u001b[0m\u001b[32m────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[32m─╮\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m                                                                                                           \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m    Answer: No, this contract does not mention any provision regarding customer data deletion.             \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m                                                                                                           \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m    Question: Based on the information provided in this contract, does the customer have the right to      \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m    request the deletion of customer data?                                                                 \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m    Context: [Your MSA content here]                                                                       \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m                                                                                                           \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m    Answer: No, the customer does not have the right to request the deletion of customer data based on     \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m    the information provided in this contract.                                                             \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m                                                                                                           \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m    Question: If the customer does not have the right to request the deletion of                           \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m│\u001b[0m                                                                                                           \u001b[32m│\u001b[0m   \n",
            "   \u001b[32m╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m   \n",
            "                                                                                                                   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from pyboxen import boxen\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "def generate_improved_response(prompt, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Generates an improved response using the provided model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The input prompt.\n",
        "        model: The language model to use for generation.\n",
        "        tokenizer: The tokenizer associated with the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response, extracted from the model's output.\n",
        "    \"\"\"\n",
        "    # Create a pipeline for text generation with the provided model and tokenizer\n",
        "    pipe = pipeline(\n",
        "        task=\"text-generation\",  # Define the task as text generation\n",
        "        model=model,  # Provide the model for text generation\n",
        "        tokenizer=tokenizer,  # Provide the tokenizer for the model\n",
        "        device=0 if torch.cuda.is_available() else -1  # Use GPU if available, otherwise use CPU\n",
        "    )\n",
        "    # Generate the response based on the prompt, limiting the output to 100 new tokens\n",
        "    response = pipe(prompt, max_new_tokens=100)[0]['generated_text']\n",
        "    # Return only the generated portion of the response (excluding the prompt)\n",
        "    return response.split(prompt)[-1].strip()  # Extract only the generated part\n",
        "\n",
        "# Test prompt: Define the prompt you want to use for testing\n",
        "test_prompt = \"\"\"\n",
        "Question: Does this contract have a provision that mentions that customer data will be deleted upon request by customer or after closure or termination of agreement for whatsoever reason?\n",
        "Context: [Your MSA content here]\n",
        "\"\"\"\n",
        "\n",
        "# Function to print a header with centered text and equal signs around it\n",
        "def print_header(text):\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(text.center(60))\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Clear memory cache before starting the inference to avoid any memory issues\n",
        "gc.collect()  # Garbage collection\n",
        "torch.cuda.empty_cache()  # Clear GPU memory cache\n",
        "\n",
        "# Use automatic mixed precision (AMP) for better performance on compatible GPUs\n",
        "with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "    # Generate the base response using the provided model and tokenizer\n",
        "    base_response = generate_improved_response(test_prompt, model, tokenizer)\n",
        "\n",
        "# Test LoRA model: Print the header for the LoRA model test\n",
        "print_header(\"Testing LoRA Model\")\n",
        "\n",
        "# Clear memory cache before running the LoRA model\n",
        "gc.collect()  # Garbage collection\n",
        "torch.cuda.empty_cache()  # Clear GPU memory cache\n",
        "\n",
        "# Use automatic mixed precision (AMP) again for better performance\n",
        "with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "    # Load the LoRA model for testing\n",
        "    lora_model = load_lora_model(model, \"./phi-1_5-lora-msa-final\")\n",
        "    # Generate the response using the LoRA model and tokenizer\n",
        "    lora_response = generate_improved_response(test_prompt, lora_model, tokenizer)\n",
        "\n",
        "# Print the LoRA model's response inside a formatted box with some styling\n",
        "print(boxen(\n",
        "    lora_response,  # The generated response\n",
        "    title=\"LoRA Model Response\",  # Title for the box\n",
        "    padding=1,  # Padding inside the box\n",
        "    margin=1,  # Margin around the box\n",
        "    color=\"green\"  # Color of the box\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRpdLPA1WPJA"
      },
      "source": [
        "## The Conclusion! 🎉\n",
        "\n",
        "After all the hard work and training, we now have an model assistant that can quickly and accurately answer questions about complex legal documents, like Master Service Agreements (MSAs)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00290924d3be460dbaf91e5c4c072994": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f56ce32d0140c78493adc5c1293e4d",
            "placeholder": "​",
            "style": "IPY_MODEL_c4570e4d13b04016958017d887d8c5a6",
            "value": " 51/0 [00:00&lt;00:00, 805.70 examples/s]"
          }
        },
        "033f8bb42cd941b3b506c9b99708063f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "044f1e76e8904092946e570384e5633d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15e5552e73ed48c0a9c72ed047070e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a7fe2bf0a4c4cb6acc42239776688af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e8a567fc0624040bbc72b6d0c05b75d",
              "IPY_MODEL_a7df026e95934cada14e7eeec7b15a6b",
              "IPY_MODEL_00290924d3be460dbaf91e5c4c072994"
            ],
            "layout": "IPY_MODEL_ffa82fea6cdf4fb5bbd6c8b776640803"
          }
        },
        "220c6892b6a947a4becc11731daca8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24961c75465a4f3394a28b7707fd78f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2535b492a8c745bfbc622f70ec284d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c63da6c028c4f3db274f15b839871f0",
              "IPY_MODEL_56d137f3df664100b27a0154f192b538",
              "IPY_MODEL_2b3949d47397473998bd38ca77885525"
            ],
            "layout": "IPY_MODEL_e1233c69531846d3ac32228cef7eeebf"
          }
        },
        "2b3949d47397473998bd38ca77885525": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74aea686b547400e9a6848e25a226145",
            "placeholder": "​",
            "style": "IPY_MODEL_68d20c88101f4c9f92a50d806d2be448",
            "value": " 51/51 [00:00&lt;00:00, 251.19 examples/s]"
          }
        },
        "32126fbd6c48409c8826de7f61dfc5fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7985b363e15f41868bcb74de0706c1a4",
            "placeholder": "​",
            "style": "IPY_MODEL_24961c75465a4f3394a28b7707fd78f0",
            "value": " 51/51 [00:00&lt;00:00, 850.69 examples/s]"
          }
        },
        "4d3b2f1dcbcf42bcb32eb6b68931b0dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d137f3df664100b27a0154f192b538": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceddbb5e420c4c7fa725f45333a1ae5b",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7640b0a8a0d14eab985008eb2d4f266d",
            "value": 51
          }
        },
        "56eee08c9ea444bc83f48804cdc4a0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "595272d9de3147b7a592867490dd7bad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849e0b8faca04dd0826c7c4ce5dc65bd",
            "placeholder": "​",
            "style": "IPY_MODEL_7b91b58e029343c09dff929d9cec9431",
            "value": "Map: 100%"
          }
        },
        "5b0ba0b7d1d8447994c8bb876bdc074b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65f56ce32d0140c78493adc5c1293e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "662d416033524da980bc95206d690e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5fe469264504cc4b2720d2fdf9ad98b",
              "IPY_MODEL_fc3c63846ff94af4b60003584f0fcb3e",
              "IPY_MODEL_daf0900d07744c1593c8c55bfad4ebdc"
            ],
            "layout": "IPY_MODEL_044f1e76e8904092946e570384e5633d"
          }
        },
        "68d20c88101f4c9f92a50d806d2be448": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c63da6c028c4f3db274f15b839871f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_220c6892b6a947a4becc11731daca8ee",
            "placeholder": "​",
            "style": "IPY_MODEL_cb0fa511c147449a9575a0c090bed367",
            "value": "Map: 100%"
          }
        },
        "74aea686b547400e9a6848e25a226145": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7640b0a8a0d14eab985008eb2d4f266d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7985b363e15f41868bcb74de0706c1a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b91b58e029343c09dff929d9cec9431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c2a7d2d0b2e4d30b22e2a33e94bb072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "849e0b8faca04dd0826c7c4ce5dc65bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "902ffdac27b3434193d92b8bb73a2629": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e8a567fc0624040bbc72b6d0c05b75d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f96bfbdeae5e46e5afc8c55519d4ca7a",
            "placeholder": "​",
            "style": "IPY_MODEL_033f8bb42cd941b3b506c9b99708063f",
            "value": "Generating train split: "
          }
        },
        "a083f5a0451240858ff544ad2a95be4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a341246c41f24983a02232bbdfd7a539": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5fe469264504cc4b2720d2fdf9ad98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0cb37d544134807ac2409feb96fedb6",
            "placeholder": "​",
            "style": "IPY_MODEL_15e5552e73ed48c0a9c72ed047070e2c",
            "value": "Generating train split: "
          }
        },
        "a7df026e95934cada14e7eeec7b15a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56eee08c9ea444bc83f48804cdc4a0a8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a083f5a0451240858ff544ad2a95be4f",
            "value": 1
          }
        },
        "a89de040f8bb499c888684e763f11f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3c17ede140c45d9a471595d45fe761f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4570e4d13b04016958017d887d8c5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb0fa511c147449a9575a0c090bed367": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceddbb5e420c4c7fa725f45333a1ae5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1cc447a26144b52a86bca568f24c75e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_595272d9de3147b7a592867490dd7bad",
              "IPY_MODEL_fa8eda38af9b4a0f9433dec1e1d75285",
              "IPY_MODEL_32126fbd6c48409c8826de7f61dfc5fc"
            ],
            "layout": "IPY_MODEL_a341246c41f24983a02232bbdfd7a539"
          }
        },
        "daf0900d07744c1593c8c55bfad4ebdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d3b2f1dcbcf42bcb32eb6b68931b0dd",
            "placeholder": "​",
            "style": "IPY_MODEL_a89de040f8bb499c888684e763f11f26",
            "value": " 51/0 [00:00&lt;00:00, 1266.89 examples/s]"
          }
        },
        "e1233c69531846d3ac32228cef7eeebf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0cb37d544134807ac2409feb96fedb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f96bfbdeae5e46e5afc8c55519d4ca7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa8eda38af9b4a0f9433dec1e1d75285": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0ba0b7d1d8447994c8bb876bdc074b",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3c17ede140c45d9a471595d45fe761f",
            "value": 51
          }
        },
        "fc3c63846ff94af4b60003584f0fcb3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c2a7d2d0b2e4d30b22e2a33e94bb072",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_902ffdac27b3434193d92b8bb73a2629",
            "value": 1
          }
        },
        "ffa82fea6cdf4fb5bbd6c8b776640803": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}