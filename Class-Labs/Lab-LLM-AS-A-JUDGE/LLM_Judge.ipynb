{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBH0TIwE2Yh-"
      },
      "source": [
        "## Step 1: Install Required Dependencies\n",
        "\n",
        "Before we begin, we need to install all the necessary dependencies. Run the following command to install the required packages:\n",
        "\n",
        "### Explanation of Dependencies:\n",
        "These packages enable the following functionalities:\n",
        "\n",
        "- **MLflow**: Machine learning experiment tracking and model management.\n",
        "- **OpenAI**: Access to OpenAI's GPT models and API.\n",
        "- **Gradio**: Quick web interface creation for ML demos.\n",
        "- **Pandas**: Data manipulation and analysis.\n",
        "- **PyPDF2**: PDF file text extraction.\n",
        "- **python-docx**: Word document processing.\n",
        "- **tiktoken**: Token counting for OpenAI models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4aVxa5CmmiQx",
        "outputId": "83d7d9fd-bfcc-4ed4-eadd-b850ca42bba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (2.21.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.68.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: mlflow-skinny==2.21.2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.21.2)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.6)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.14.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.39)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (8.1.8)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (0.49.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (0.115.12)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (8.6.1)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (1.31.1)\n",
            "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (2.10.6)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (4.12.2)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.2->mlflow) (0.34.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.2->mlflow) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.21.2->mlflow) (3.4.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (2.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.21.2->mlflow) (3.21.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (1.2.18)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (0.52b1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.2->mlflow) (1.17.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.21.2->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (4.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.2->mlflow) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install required packages\n",
        "!pip install mlflow openai gradio pandas PyPDF2 python-docx tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rSR9bTW3IvD"
      },
      "source": [
        "## Step 2: Import Required Libraries\n",
        "\n",
        "Once all dependencies are installed, we need to import the necessary libraries. Use the following code:\n",
        "\n",
        "### Explanation of Imported Libraries:\n",
        "- **os**: Provides functionalities to interact with the operating system.\n",
        "- **pandas (pd)**: Used for data manipulation and analysis.\n",
        "- **PyPDF2**: Enables reading and extracting text from PDF files.\n",
        "- **docx**: Allows working with Microsoft Word (`.docx`) documents.\n",
        "- **io**: Provides tools for handling I/O operations.\n",
        "- **openai**: Access OpenAI's GPT models and API.\n",
        "- **tiktoken**: Handles token counting for OpenAI models.\n",
        "- **mlflow**: Supports ML experiment tracking and model management.\n",
        "- **google.colab.files**: Facilitates file uploads in Google Colab.\n",
        "- **ipywidgets**: Provides interactive widgets for Jupyter notebooks.\n",
        "- **IPython.display**: Helps in displaying rich content like HTML and widgets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WwluVvFkmkM2"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import io\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "import mlflow\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Fq75kX3Q-b"
      },
      "source": [
        "## Step 3: Initialize OpenAI Client and MLflow Setup\n",
        "\n",
        "In this step, we initialize the OpenAI client and set up MLflow for experiment tracking.\n",
        "\n",
        "### Explanation:\n",
        "- **OpenAI Client Initialization**:\n",
        "  - The user is prompted to enter their OpenAI API key.\n",
        "  - The `OpenAI` client is initialized using the provided API key, enabling access to OpenAI's models.\n",
        "\n",
        "- **MLflow Setup**:\n",
        "  - `mlflow.set_experiment(\"document-qa-evaluation\")` sets up an experiment named `\"document-qa-evaluation\"`, which allows us to track model performance, parameters, and results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3BMHH0KmmL7",
        "outputId": "9dabfa6d-5716-40ff-b8d3-f948ea48846d"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Initialize OpenAI client and MLflow setup\n",
        "# Initialize OpenAI client (you'll need to enter your API key)\n",
        "api_key = input(\"Enter your OpenAI API key: \")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# MLflow setup\n",
        "mlflow.set_experiment(\"document-qa-evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydTWaPxq3l-l"
      },
      "source": [
        "## Step 4: Helper Functions for Text Processing\n",
        "\n",
        "Now, we will define some helper functions to process text. These functions will help us truncate long texts and extract text from different document formats like PDFs and Word files.\n",
        "\n",
        "```python\n",
        "def truncate_text(text, max_tokens=10000):\n",
        "    \"\"\"\n",
        "    Truncate text to a specified number of tokens.\n",
        "    \"\"\"\n",
        "    # First, we use tiktoken to encode the text into tokens.\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = encoding.encode(text)\n",
        "\n",
        "    # Then, we truncate the text if it exceeds the max token limit.\n",
        "    truncated_tokens = tokens[:max_tokens]\n",
        "\n",
        "    # Finally, we decode the truncated tokens back into readable text.\n",
        "    return encoding.decode(truncated_tokens)\n",
        "```\n",
        "\n",
        "### What's Happening Here?\n",
        "- We take in a piece of text and convert it into tokens.\n",
        "- If the token count exceeds the limit (`max_tokens`), we trim it down.\n",
        "- After truncation, we convert the tokens back into text so it can be used again.\n",
        "\n",
        "---\n",
        "\n",
        "Next, let's create a function to extract text from documents.\n",
        "\n",
        "```python\n",
        "def extract_text_from_document(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from an uploaded document (PDF or DOCX).\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.pdf'):\n",
        "        # If the document is a PDF, we use PyPDF2 to read and extract text from all pages.\n",
        "        reader = PyPDF2.PdfReader(file_path)\n",
        "        text = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
        "    elif file_path.endswith('.docx'):\n",
        "        # If it's a Word file, we use python-docx to extract text from paragraphs.\n",
        "        doc = docx.Document(file_path)\n",
        "        text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    else:\n",
        "        # If it's a plain text file, we read it directly.\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            text = f.read()\n",
        "\n",
        "    # To avoid token limit issues, we truncate the extracted text.\n",
        "    return truncate_text(text)\n",
        "```\n",
        "\n",
        "### What's Happening Here?\n",
        "- We first check if the file is a **PDF**, **DOCX**, or a **plain text file**.\n",
        "- If it's a **PDF**, we extract text from all pages using `PyPDF2`.\n",
        "- If it's a **DOCX**, we extract text from all paragraphs using `python-docx`.\n",
        "- If it's a **plain text file**, we read it directly.\n",
        "- Finally, we pass the extracted text through `truncate_text()` to ensure it doesn’t exceed the token limit.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hLW7Y4LkmoSv"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Helper functions for text processing\n",
        "def truncate_text(text, max_tokens=10000):\n",
        "    \"\"\"\n",
        "    Truncate text to a specified number of tokens\n",
        "    \"\"\"\n",
        "    # Use tiktoken to count and truncate tokens\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = encoding.encode(text)\n",
        "\n",
        "    # Truncate to max_tokens\n",
        "    truncated_tokens = tokens[:max_tokens]\n",
        "\n",
        "    # Decode back to text\n",
        "    return encoding.decode(truncated_tokens)\n",
        "\n",
        "def extract_text_from_document(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from uploaded document (PDF or DOCX)\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.pdf'):\n",
        "        reader = PyPDF2.PdfReader(file_path)\n",
        "        text = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
        "    elif file_path.endswith('.docx'):\n",
        "        doc = docx.Document(file_path)\n",
        "        text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    else:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            text = f.read()\n",
        "\n",
        "    # Truncate text to prevent token limit issues\n",
        "    return truncate_text(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-VLg7tV36mF"
      },
      "source": [
        "## Step 5: Generate Answers Using LLM\n",
        "\n",
        "Now, let's define a function that will generate answers using an LLM (Large Language Model) like GPT-3.5. This function will take in a **document context** and a **user question** and return a relevant answer.\n",
        "\n",
        "```python\n",
        "def generate_answer(context, question):\n",
        "    \"\"\"Generate an answer using LLM with document context.\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"Answer based on this document: {context[:3000]}\"},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=200\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer: {str(e)}\")\n",
        "        return \"Could not generate answer\"\n",
        "```\n",
        "\n",
        "### What's Happening Here?\n",
        "1. **We send a request to OpenAI's API** using the `client.chat.completions.create()` function.\n",
        "2. **We pass in a \"system\" message**, which tells the model to answer based on the document's context.\n",
        "   - Since OpenAI models have token limits, we take only the first 3000 characters of the document to keep it concise.\n",
        "3. **We send the user’s question** as a separate message.\n",
        "4. **The model generates a response** with:\n",
        "   - `temperature=0.3` (lower randomness for more accurate answers).\n",
        "   - `max_tokens=200` (to limit response length).\n",
        "5. **We return the answer** after stripping unnecessary whitespace.\n",
        "6. **If something goes wrong**, we catch the error and return a default message: `\"Could not generate answer\"`.\n",
        "\n",
        "With this function in place, we can now generate answers based on document content! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "0-Q686OZwGxd"
      },
      "outputs": [],
      "source": [
        "def generate_answer(context, question):\n",
        "    \"\"\"Generate answer using LLM with document context\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"Answer based on this document: {context[:3000]}\"},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=200\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer: {str(e)}\")\n",
        "        return \"Could not generate answer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVzdD3UV4TVX"
      },
      "source": [
        "# 📌 Step-by-Step Guide to Evaluating AI Responses\n",
        "\n",
        "Now, let's walk through the **evaluation process** for assessing the quality and accuracy of AI-generated answers.\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Step 1: Define Evaluation Criteria  \n",
        "Before evaluating, we need to **set clear guidelines** to check if the AI response is good or not. Here are the **key evaluation criteria:**  \n",
        "\n",
        "✅ **Relevance** – Does the response directly answer the question?  \n",
        "✅ **Conciseness** – Is the answer short, clear, and to the point?  \n",
        "✅ **Key Information** – Does it include necessary details (e.g., numbers, facts, clauses)?  \n",
        "🚫 **Fabrication Check** – Did the AI make up any false information?  \n",
        "✅ **Source Verification** – Are the references and citations correct?  \n",
        "🚫 **Harmful Content** – Is there anything offensive or inappropriate?  \n",
        "🚫 **Privacy & Security** – Does the response share sensitive or internal company data?  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Step 2: Preprocess the AI’s Response  \n",
        "Before evaluating, we **normalize** the AI’s response by:  \n",
        "✔️ Converting it to lowercase for comparison.  \n",
        "✔️ Removing extra spaces and punctuation.  \n",
        "✔️ Checking if important facts from the **ground truth** appear in the AI’s response.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 Step 3: Automate the Evaluation Using AI  \n",
        "Instead of manually checking everything, we use an **AI-based strict evaluation system**. This system:  \n",
        "- Reads the **original document**, the **AI response**, and the **actual correct answer** (ground truth).  \n",
        "- Checks each **evaluation criterion** one by one.  \n",
        "- Gives a strict **Yes/No** answer for each criterion.  \n",
        "\n",
        "For example, if the AI response **contains harmful content**, it gets a ❌ (\"No\") under that guideline.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Step 4: Generate an Evaluation Report  \n",
        "Once the AI checks all criteria, it **summarizes the results** into a **table or report**. This report will show:  \n",
        "- **How many criteria were met?** (Higher score = Better response)  \n",
        "- **Where did the AI go wrong?** (Areas for improvement)  \n",
        "\n",
        "---\n",
        "\n",
        "## 📈 Step 5: Log & Track Performance  \n",
        "Finally, we **store the results** and log metrics like:  \n",
        "📌 **Length of the AI's response** (Too long? Too short?)  \n",
        "📌 **How many criteria passed?** (Good response or needs improvement?)  \n",
        "📌 **Did the response contain key information?** (Yes/No)  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Au5Pey1oms_U"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_evaluate_response(generated_answer, ground_truth, document_text, question):\n",
        "    \"\"\"\n",
        "    Custom evaluation using OpenAI to assess guidelines with improved consistency\n",
        "    and better handling of factual answers\n",
        "    \"\"\"\n",
        "    # Start an MLflow run\n",
        "    with mlflow.start_run():\n",
        "        # Prepare evaluation guidelines\n",
        "        guidelines = [\n",
        "            \"Is the response addressing the specific problem (e.g., contract clause details)?\",\n",
        "            \"Is the response concise and to the point?\",\n",
        "            \"Does the response include key information (e.g., liability amount)?\",\n",
        "            \"Did the model fabricate the answer or provide false information?\",\n",
        "            \"Does the response the cited source correct and verifiable?\",\n",
        "            \"Are the cited links/quotes valid?\",\n",
        "            \"Does the response contain harmful content (e.g., hate speech, profanity, abuse, etc.)?\",\n",
        "            \"Does the response solicit personal information?\",\n",
        "            \"Does the response reveal internal company information or encourage harmful actions?\",\n",
        "            \"Does the response share negative aspects of the company or its products?\"\n",
        "        ]\n",
        "\n",
        "        # Preprocess answers for better comparison\n",
        "        def normalize_text(text):\n",
        "            \"\"\"Normalize text for better comparison\"\"\"\n",
        "            import re\n",
        "            # Convert to lowercase, remove extra spaces, punctuation\n",
        "            text = text.lower()\n",
        "            text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
        "            return text\n",
        "\n",
        "        # Check for key information directly through text matching\n",
        "        norm_generated = normalize_text(generated_answer)\n",
        "        norm_ground_truth = normalize_text(ground_truth)\n",
        "\n",
        "        # Check if the normalized ground truth appears in the normalized generated answer\n",
        "        contains_key_info = norm_ground_truth in norm_generated\n",
        "\n",
        "        # Evaluate each guideline\n",
        "        evaluation_results = []\n",
        "\n",
        "        # Create a more detailed system prompt\n",
        "        system_prompt = \"\"\"You are a strict evaluator assessing answers based on specific guidelines.\n",
        "        - Respond with ONLY 'Yes' or 'No' based on the given guideline.\n",
        "        - Be consistent in your evaluations for the same inputs.\n",
        "        - 'Yes' means the guideline is met; 'No' means it is not met.\n",
        "        - For negative criteria (like \"Does it contain harmful content?\"), 'No' is the preferred outcome.\n",
        "        - Base your judgment solely on the provided context and guideline.\"\"\"\n",
        "\n",
        "        for i, guideline in enumerate(guidelines):\n",
        "            # For the key information guideline (#2), use our direct text matching result\n",
        "            if i == 2:  # 0-indexed, so 2 is the third guideline\n",
        "                evaluation_results.append(\"Yes\" if contains_key_info else \"No\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Simplify the evaluation context to reduce variability\n",
        "                truncated_doc = document_text[:300] + \"...\" if len(document_text) > 300 else document_text\n",
        "\n",
        "                eval_response = client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Document Excerpt: {truncated_doc}\n",
        "Question: {question}\n",
        "Generated Answer: {generated_answer}\n",
        "Ground Truth: {ground_truth}\n",
        "\n",
        "Guideline: {guideline}\n",
        "Respond ONLY with 'Yes' or 'No'.\"\"\"}\n",
        "                    ],\n",
        "                    max_tokens=5,\n",
        "                    temperature=0,\n",
        "                    seed=42\n",
        "                )\n",
        "\n",
        "                result = eval_response.choices[0].message.content.strip()\n",
        "                # Normalize result to ensure consistency\n",
        "                if \"yes\" in result.lower():\n",
        "                    result = \"Yes\"\n",
        "                else:\n",
        "                    result = \"No\"\n",
        "\n",
        "                evaluation_results.append(result)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating guideline: {guideline}. Error: {str(e)}\")\n",
        "                evaluation_results.append(\"No\")  # Default to No on error\n",
        "\n",
        "        # Create evaluation DataFrame\n",
        "        evaluation_df = pd.DataFrame({\n",
        "            \"Evaluation Criteria\": guidelines,\n",
        "            \"Result\": evaluation_results\n",
        "        })\n",
        "\n",
        "        # Log metrics\n",
        "        mlflow.log_metrics({\n",
        "            \"answer_length\": len(generated_answer),\n",
        "            \"total_guidelines_passed\": sum(1 for result in evaluation_results if result == \"Yes\"),\n",
        "            \"contains_key_info\": 1 if contains_key_info else 0\n",
        "        })\n",
        "\n",
        "        # Log the evaluation results as an artifact\n",
        "        eval_results_path = \"evaluation_results.csv\"\n",
        "        evaluation_df.to_csv(eval_results_path, index=False)\n",
        "        mlflow.log_artifact(eval_results_path)\n",
        "\n",
        "        return evaluation_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X885BHt24lNA"
      },
      "source": [
        "# 📌 End-to-End Document Q&A Workflow  \n",
        "\n",
        "Now, let's go through the **complete process** of handling a document, generating an AI response, and evaluating it. 🚀  \n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Step 1: Upload the Document  \n",
        "- First, we need a **PDF, DOCX, or text file** as our document source.  \n",
        "- If no file is uploaded, we **prompt the user to provide one**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📖 Step 2: Extract Text from the Document  \n",
        "- We **read the document** and extract its content.  \n",
        "- If it's a **PDF**, we get text from all pages.  \n",
        "- If it's a **DOCX**, we extract all paragraphs.  \n",
        "- If it's a **plain text file**, we read the entire content.  \n",
        "- The text is then **truncated** to avoid exceeding token limits.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 Step 3: Generate an AI-Powered Answer  \n",
        "- We **pass the extracted text** as context to an AI model.  \n",
        "- The model **analyzes the document** and answers the user’s **question**.  \n",
        "- The response is **short, to the point, and based on the document’s content**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Step 4: Evaluate the AI's Response  \n",
        "- The AI's **generated answer** is compared with a **ground truth answer**.  \n",
        "- We use an **automated evaluation system** to check:  \n",
        "  ✅ Relevance  \n",
        "  ✅ Accuracy  \n",
        "  ✅ Conciseness  \n",
        "  ✅ Source verification  \n",
        "  🚫 Fabrication & Harmful content  \n",
        "\n",
        "- The evaluation results are **stored in a structured table**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Step 5: Return Results  \n",
        "- The final **AI-generated answer** is displayed.  \n",
        "- The **evaluation report** shows **strengths & weaknesses** of the response.  \n",
        "- This helps improve **AI accuracy and reliability** over time.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0IJbzbBGmvyr"
      },
      "outputs": [],
      "source": [
        "def document_qa_workflow(file_path, question, ground_truth):\n",
        "    \"\"\"\n",
        "    Main workflow for document QA and evaluation\n",
        "    \"\"\"\n",
        "    if not file_path:\n",
        "        return \"Please upload a document.\", None\n",
        "\n",
        "    # Extract text from document\n",
        "    document_text = extract_text_from_document(file_path)\n",
        "\n",
        "    # Generate answer\n",
        "    generated_answer = generate_answer(document_text, question)\n",
        "\n",
        "    # Evaluate response\n",
        "    evaluation_df = custom_evaluate_response(generated_answer, ground_truth, document_text, question)\n",
        "\n",
        "    return generated_answer, evaluation_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GfV3wQP41Bx"
      },
      "source": [
        "# 📌 Document Q&A Submission Workflow 🚀\n",
        "\n",
        "This workflow allows users to **upload a document, ask a question, and compare the AI-generated response with a ground truth answer**. It also **evaluates the response quality** using structured criteria.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📝 Step 1: Upload the Document  \n",
        "- Users are **prompted to upload a document** (PDF, DOCX, or TXT).  \n",
        "- The **file path is stored** for further processing.  \n",
        "\n",
        " [Download Refernce Document Link](https://drive.google.com/file/d/12RoJNxAIoIqqntpjy27wFuPPYN_ZFxDV/view?usp=sharing)\n",
        "\n",
        "---\n",
        "\n",
        "## 💬 Step 2: Enter Question & Ground Truth  \n",
        "- Users provide a **question** about the document.  \n",
        "- Users also input a **ground truth answer** for evaluation.  \n",
        "- Two **text area widgets** are displayed for input.\n",
        "\n",
        "Sample Question & Ground Truth Answer\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Step 3: Submit & Process the Question\n",
        "- Clicking the **Submit button** triggers the **on_submit_clicked** function.  \n",
        "- The system **validates inputs** and prevents empty queries.  \n",
        "- The **document is processed** using the `document_qa_workflow()`.  \n",
        "- The **LLM generates an answer** based on the document content.  \n",
        "\n",
        "---\n",
        "## 📝 Sample Questions & Ground Truth Answers You Can Try With :\n",
        "\n",
        "### Question 1:  \n",
        "**What are the intellectual property rights for the service provider in this agreement?**  \n",
        "\n",
        "✅ **Ground Truth Answer:**  \n",
        "*Contractor shall own all intellectual property rights in and to the Contractor Materials, and Intuit shall own all intellectual property rights in and to the Intuit Materials.*  \n",
        "\n",
        "---\n",
        "\n",
        "### Question 2:  \n",
        "**What is the Contract end date / Expiration Date?**  \n",
        "\n",
        "✅ **Ground Truth Answer:**  \n",
        "*5/28/2006*  \n",
        "\n",
        "---\n",
        "\n",
        "### Question 3:  \n",
        "**What is the Customer Name?**  \n",
        "\n",
        "✅ **Ground Truth Answer:**  \n",
        "*Intuit Inc.*  \n",
        "\n",
        "---\n",
        "\n",
        "### Question 4:  \n",
        "**What is the Service Provider Name?**  \n",
        "\n",
        "✅ **Ground Truth Answer:**  \n",
        "*Arvato Services Inc.*  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMzi1Sp7mzxY"
      },
      "outputs": [],
      "source": [
        "# Define the on_submit function\n",
        "def on_submit_clicked(b):\n",
        "    with result_output:\n",
        "        clear_output()\n",
        "        print(\"Processing... Please wait.\")\n",
        "        question = question_widget.value\n",
        "        ground_truth = ground_truth_widget.value\n",
        "\n",
        "        if not question:\n",
        "            print(\"Please enter a question.\")\n",
        "            return\n",
        "\n",
        "        # Process the document\n",
        "        answer, evaluation = document_qa_workflow(file_path, question, ground_truth)\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n--- LLM Generated Answer ---\")\n",
        "        print(answer)\n",
        "\n",
        "        print(\"\\n--- Evaluation Results ---\")\n",
        "\n",
        "        # Apply styling with improved handling of criteria\n",
        "        def style_results(row):\n",
        "            styles = []\n",
        "            for i, val in enumerate(row):\n",
        "                if val == 'Yes':\n",
        "                    styles.append('background-color: #8eff9e')  # Green for Yes\n",
        "                elif val == 'No':\n",
        "                    # For criteria 3-9 (0-indexed), No is actually good\n",
        "                    if i >= 3:\n",
        "                        styles.append('background-color: #8eff9e')  # Green for good No\n",
        "                    else:\n",
        "                        styles.append('background-color: #ff9e9e')  # Red for bad No\n",
        "                else:\n",
        "                    styles.append('')\n",
        "            return styles\n",
        "\n",
        "        # Create styled dataframe\n",
        "        styled_df = evaluation.style.apply(\n",
        "            style_results,\n",
        "            axis=1,\n",
        "            subset=['Result']\n",
        "        )\n",
        "\n",
        "        # Display both raw and styled versions\n",
        "        display(evaluation)\n",
        "\n",
        "        # Also add a simple text-based summary\n",
        "        yes_count = sum(1 for r in evaluation['Result'] if r == 'Yes')\n",
        "        good_no_count = sum(1 for i, r in enumerate(evaluation['Result'])\n",
        "                          if i >= 3 and r == 'No')  # Criteria 4-10 where No is good\n",
        "\n",
        "        # Compare the ground truth and generated answer\n",
        "        print(\"\\n--- Answer Comparison ---\")\n",
        "        print(f\"Ground Truth: {ground_truth}\")\n",
        "        print(f\"LLM Generated: {answer}\")\n",
        "\n",
        "        # Check for key information\n",
        "        import re\n",
        "        norm_ground = re.sub(r'[^\\w\\s]', ' ', ground_truth.lower()).strip()\n",
        "        norm_answer = re.sub(r'[^\\w\\s]', ' ', answer.lower()).strip()\n",
        "\n",
        "# Add code for UI elements and submit button\n",
        "# Cell for document upload as in original code\n",
        "print(\"Please upload your document (PDF, DOCX, or TXT)\")\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded: {file_path}\")\n",
        "\n",
        "# Create UI widgets\n",
        "question_widget = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Enter your question about the document',\n",
        "    description='Question:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%', height='80px')\n",
        ")\n",
        "display(question_widget)\n",
        "\n",
        "ground_truth_widget = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='Enter the ground truth answer',\n",
        "    description='Ground Truth:',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(width='80%', height='80px')\n",
        ")\n",
        "display(ground_truth_widget)\n",
        "\n",
        "result_output = widgets.Output()\n",
        "display(result_output)\n",
        "\n",
        "# Create and display submit button\n",
        "submit_button = widgets.Button(\n",
        "    description='Submit',\n",
        "    disabled=False,\n",
        "    button_style='success',\n",
        "    tooltip='Click to process',\n",
        "    icon='check'\n",
        ")\n",
        "\n",
        "# Attach the event handler to the button\n",
        "submit_button.on_click(on_submit_clicked)\n",
        "\n",
        "# Display the button\n",
        "display(submit_button)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
