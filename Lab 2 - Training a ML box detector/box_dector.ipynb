{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07969d47",
   "metadata": {},
   "source": [
    "## Getting the DATA ready\n",
    "\n",
    "First part of getting the model ready is to gather data. You can get the data from Open data sources, or from the internet using scrapping tools.\n",
    "\n",
    "To get started quickly you can access this `https://drive.google.com/drive/folders/11DmMCKiQLfhzy59vbzeEN_D-LZtAi1BW?usp=sharing` data that we have scraped and made public.\n",
    "\n",
    "\n",
    "To get high quality images from google we are using a third party service `Serpapi`. \n",
    "\n",
    "# Setting up and running the scraper\n",
    "To get started with the scraper we first need to get an API key from `serpapi`, we need this so that our API request can be authenticated.\n",
    "\n",
    "Follow these steps to generate your own serpapi API key:\n",
    "\n",
    "- Step 1: Go to `https://serpapi.com/`.\n",
    "  \n",
    "  `insert step 1 here`\n",
    "\n",
    "- Step 2: Register yourself on the platform.\n",
    "  \n",
    "  `insert step 2 here`\n",
    "\n",
    "- Step 3: You should see an API key on the dashboard.\n",
    "  \n",
    "  `insert step 3 here`\n",
    "\n",
    "- Step 4: Copy that key that's what we need.\n",
    "  \n",
    "  `insert step 4 here`\n",
    "\n",
    "- Step 5: Add the key you just copied to the .env file.\n",
    "  \n",
    "  `insert step 5 here`\n",
    "\n",
    "- Step 6: If you haven't installed the project requirements install them from the requirements.txt file or by simply running `$ pip install -r requirements.txt\n",
    "`.\n",
    "\n",
    "- Step 7: Inside `get_hd_images_serpapi.py` add your search query for the images you want.\n",
    "  \n",
    "  `insert step 7 here`\n",
    "\n",
    "- Step 8: Now run the code and wait till all the processing is done. Run by typing `py get_hd_images_serpapi.py` in your terminal. \n",
    "  \n",
    "  `insert step 8 here`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17b10d95",
   "metadata": {},
   "source": [
    "## Training and testing your model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33f6189a",
   "metadata": {},
   "source": [
    "### To get the trainer ready\n",
    "To train the custom vision model, follow this official guide: https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/get-started-build-detector\n",
    "\n",
    "### Getting your camera ready\n",
    "1. Get any camera that can sed RTSP stream over network we used \n",
    "   \n",
    "a) This camera earlier version but this should work (https://www.wyze.com/products/wyze-cam?related_selling_plan=41618559008930)\n",
    "\n",
    "b) Further here are instruction to setup RTSP streaming over this camera, this will enable you to broadcast the camera output over your network.We will use this RTSP stream to take frame from camera and send to our model for inference. (https://support.wyze.com/hc/en-us/articles/360026245231-Wyze-Cam-RTSP)\n",
    "\n",
    "c) Test if your camera stream is accessible using VLC media player as per instruction here : https://www.thewindowsclub.com/how-to-play-rtsp-stream-in-vlc-media-player\n",
    "\n",
    "d) Change the SOURCE section in your .env with your <RTSP stream address>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a40817f6",
   "metadata": {},
   "source": [
    "### Run this code to start processing your camera stream and get an SMS when ever you get object detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os, time\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import BlobServiceClient, PublicAccess, BlobType, generate_blob_sas, BlobSasPermissions\n",
    "from azure.storage.queue import QueueServiceClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n",
    "from azure.cognitiveservices.vision.customvision.training.models import CustomVisionErrorException\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from twilio.rest import Client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccb5fb7a",
   "metadata": {},
   "source": [
    "Setting up environment variables and client. Refer here `https://github.com/initmahesh/MLAI-community-labs#in-a-nutshell` to know what each variable means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003956ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "CONNECTION_STRING = os.getenv('CONNECTION_STRING').strip()\n",
    "SOURCE = os.getenv('SOURCE')\n",
    "TIME_DELAY = int(os.getenv('TIME_DELAY'))\n",
    "MANUAL_MODE = int(os.getenv('MANUAL_MODE'))\n",
    "ACCOUNT_SID = os.environ['TWILIO_ACCOUNT_SID']\n",
    "AUTH_TOKEN = os.environ['TWILIO_AUTH_TOKEN']\n",
    "PHONE_NUMBER = os.environ['TWILIO_PHONE_NUMBER']\n",
    "PREDICTION_KEY = os.environ['PREDICTION_KEY']\n",
    "ENDPOINT = os.environ['ENDPOINT_CUSTOM_VISION']\n",
    "ACCOUNT_NAME = os.environ['ACCOUNT_NAME']\n",
    "STORAGE_ACCOUNT_KEY = os.environ['STORAGE_ACCOUNT_KEY']\n",
    "prediction_credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": PREDICTION_KEY})\n",
    "predictor = CustomVisionPredictionClient(ENDPOINT, prediction_credentials)\n",
    "\n",
    "#For future extension\n",
    "# TRAINING_KEY = os.environ['TRAINING_KEY']\n",
    "# PROJECT_ID = os.environ['PROJECT_ID']\n",
    "\n",
    "# training_credentials = ApiKeyCredentials(in_headers={\"Training-key\": TRAINING_KEY})\n",
    "# trainer = CustomVisionTrainingClient(ENDPOINT, training_credentials)\n",
    "# todo : change to client_twilio \n",
    "twilio_client = Client(ACCOUNT_SID, AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d38c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = CONNECTION_STRING\n",
    "source = SOURCE\n",
    "time_delay = TIME_DELAY\n",
    "manual_mode = MANUAL_MODE\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_name = None \n",
    "queue_service = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58177331",
   "metadata": {},
   "source": [
    "Creating a blob named 'fromcamera + current date time' (Concatinating two values into a string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0cfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "container_name = 'fromcamera' + timestr\n",
    "blob_service_client.create_container(container_name)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "container_client.set_container_access_policy(signed_identifiers={}, public_access=PublicAccess.Container)\n",
    "queue_service = QueueServiceClient.from_connection_string(connection_string)\n",
    "queue_service.create_queue('fromcamera' + timestr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04969032",
   "metadata": {},
   "source": [
    "Inference function:\n",
    "- Gets the iteration name from custom vision.\n",
    "- Converts the image to bytes -> compresses -> and stores it in the memory.\n",
    "- Returns results recieved from custom vision prediction client that we created in [2] as 'predictor'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference (frame):\n",
    "    # Get the project by project_id\n",
    "    try:\n",
    "        iterations = trainer.get_iterations(PROJECT_ID)\n",
    "        published_iteration = next(iteration for iteration in iterations if iteration.publish_name)\n",
    "        publish_iteration_name = published_iteration.publish_name\n",
    "    except StopIteration:\n",
    "        print(\"No published iteration found. Please publish an iteration in the Custom Vision portal.\")\n",
    "        exit(1)\n",
    "\n",
    "    image_jpg = cv2.imencode('.jpg',frame)[1].tobytes()\n",
    "    results = predictor.detect_image(PROJECT_ID, publish_iteration_name, image_jpg)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88afa245",
   "metadata": {},
   "source": [
    "Upload frame function:\n",
    "- Takes the frame, iterator as 'i', and an optional suffix.\n",
    "- Stores the frame in memory 'image_jpg'.\n",
    "- Creats the blob name (adds the suffix at the end of the name if provided).\n",
    "- Creates a client to upload image to storage 'blob_client'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f69942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_frame(frame, i, sufix=''):\n",
    "    print(\"frame capture function returned :: \" +  str(frame is not None) + \" storing to container :: \" + container_name)\n",
    "    image_jpg = cv2.imencode('.jpg',frame)[1].tobytes()\n",
    "    blob_name= 'image' + str(i) + sufix +'.jpg' if sufix else 'image' + str(i) +'.jpg'\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "    blob_client.upload_blob(image_jpg, blob_type=BlobType.BlockBlob)\n",
    "    print(\"Total files stored :: \" + str(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "395efb62",
   "metadata": {},
   "source": [
    "'save_to_filesystem' function saves the frame as jpg to local storage, and sets the frame name to current date time before storing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ff1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_filesystem(frame):\n",
    "    #Stores frame as jpg locally\n",
    "    current_time = time.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "    local_image_location = os.path.join(os.path.join(os.path.dirname('p='), \"test/\"))\n",
    "    cv2.imwrite(f\"{local_image_location}/{current_time}.jpg\", frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee7e11fd",
   "metadata": {},
   "source": [
    "Send SMS function sends a SMS alert to the user with the image url from storage:\n",
    "\n",
    "  Creates an empty list -> Gets all the blob from storage -> Checks if blob name has 'without' in it (as we are storing two images one with bounding box and one without bounding box) -> If 'without' not found in name, adds it to the list -> Sorts the blobs in the list by timestamp -> Generates a SAS token for the [0]th index image -> then creates a sas_url\n",
    "- body: 'string' that you want to send in the SMS.\n",
    "- from: phone number given by twilio.\n",
    "- to: recipents phone number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_sms():\n",
    "    blob_list = []\n",
    "\n",
    "    for blob in container_client.list_blobs():\n",
    "        if 'without' in blob.name:\n",
    "            continue\n",
    "        blob_list.append(blob)  \n",
    "\n",
    "\n",
    "    sorted_list = sorted(blob_list, key=lambda e: e.creation_time, reverse=True)\n",
    "    sas_i = generate_blob_sas(\n",
    "        account_name= ACCOUNT_NAME,\n",
    "        container_name= container_name,\n",
    "        blob_name= sorted_list[0].name,\n",
    "        account_key= STORAGE_ACCOUNT_KEY,\n",
    "        permission= BlobSasPermissions(read=True),\n",
    "        expiry= datetime.utcnow() + timedelta(hours=8760)\n",
    "        )\n",
    "    sas_url = 'https://' + ACCOUNT_NAME +'.blob.core.windows.net/' + container_name + '/' + sorted_list[0].name + '?' + sas_i\n",
    "\n",
    "    message = twilio_client.messages.create(\n",
    "                        body=f'A Box is detected at you door! In case you are out you can view the image here: {sas_url}',\n",
    "                        from_= PHONE_NUMBER,\n",
    "                        to='+917303879964'\n",
    "                    )\n",
    "    print(sas_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf968062",
   "metadata": {},
   "source": [
    "1. Checking the source, if source is usb then stream will be captured from the webcam else a .mp4 video path can be provided.\n",
    "2. Checking the manual mode, if manual_mode = 0 frames will be sent automatically else frames can be sent manually from camera preview.\n",
    "3. Sending the frame to 'Infrance' function (created on [5]) which returns the predictions.\n",
    "4. Setting the Probability threshold as 80%.\n",
    "5. Looping through each prediction, and checking the probability, so that, all the predictions having a probability > 80% get skiped.\n",
    "6. Adding a bounding overley over detected object.\n",
    "7. Uploading the detected frame using upload_frame function (created on [6]).\n",
    "8. Saving the frame as jpg using save_to_filesystem function (created on [7]).\n",
    "9. Sending notification to user using send_sms function (created on [8]).\n",
    "10. Adding the time_delay to pause the code for the defined time_delay in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if source is not None:\n",
    "    if source == 'usb':\n",
    "        cap = cv2.VideoCapture(0)\n",
    "    else:\n",
    "        cap = cv2.VideoCapture(source)\n",
    "else:\n",
    "    print(\"Please set the value of SOURCE variable in .env file\")\n",
    "    exit()\n",
    "\n",
    "ret = True\n",
    "i = 0\n",
    "print('Created stream')\n",
    "\n",
    "while ret:\n",
    "    ret, frame = cap.read()\n",
    "    if(frame is None):\n",
    "        print(\"Unable to capture frame from source :\" + source)\n",
    "        print(\"Please check correct SOURCE variable is set in .env file\")\n",
    "        break  \n",
    "\n",
    "    if(manual_mode == 1):\n",
    "        window_name = \"Press SPACE to capture or ESC to quit\"\n",
    "        cv2.namedWindow(window_name,cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow(window_name, frame)\n",
    "              \n",
    "        k = cv2.waitKey(1)\n",
    "        if k%256 == 27:\n",
    "            # ESC pressed\n",
    "            print(\"Escape hit, closing...\")\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "                 \n",
    "        elif k%256 == 32:\n",
    "            # SPACE pressed\n",
    "            i+=1 \n",
    "            upload_frame(frame, i)\n",
    "    else:   \n",
    "        ret, frame = cap.read()\n",
    "        i+=1\n",
    "        results = inference(frame)\n",
    "        # Set the probability threshold (e.g., 0.8 for 80%)\n",
    "        probability_threshold = 0.8\n",
    "\n",
    "        # Display the results.\n",
    "        # The bounding box values are normalized, which means they are in the range of 0 to 1 relative to the image dimensions.\n",
    "        # To get the actual pixel coordinates, you can multiply these values by the width and height of the image, respectively.\n",
    "        for prediction in results.predictions:\n",
    "            if prediction.tag_name == 'Box':\n",
    "                if prediction.probability >= probability_threshold:\n",
    "                    url = f'https://mlcohort.blob.core.windows.net/{container_name}/image{i}.jpg'\n",
    "\n",
    "                    upload_frame(frame, i, '_without_overlay')\n",
    "\n",
    "                    #Storing bounding_box coordinates as x an y axis\n",
    "                    x = int(prediction.bounding_box.left * frame.shape[0])\n",
    "                    y = int(prediction.bounding_box.top * frame.shape[1])\n",
    "\n",
    "                    width = x + int(prediction.bounding_box.width * frame.shape[0])\n",
    "                    height = y + int(prediction.bounding_box.height * frame.shape[1])\n",
    "\n",
    "                    #Adding bounding_box to the frame\n",
    "                    frame = cv2.rectangle(frame, (x, y), (width, height), (0, 0, 255), 2)\n",
    "                    #Adding tag_name that we got from prediction in the bounding_box\n",
    "                    frame = cv2.putText(frame, prediction.tag_name, (x + 5, y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 1, cv2.LINE_AA, False)\n",
    "                       \n",
    "                    #TODO\n",
    "                     #extract url from blob instead of hardcoding\n",
    "                     #check for prediction tag_name\n",
    "                     #fix bounding box alignment\n",
    "                     #upload both bounding-box image and plane image\n",
    "                       \n",
    "                    print(\"\\t\" + prediction.tag_name + \": {0:.2f}% bbox.left = {1:.2f}, bbox.top = {2:.2f}, bbox.width = {3:.2f}, bbox.height = {4:.2f}\".format(prediction.probability * 100, prediction.bounding_box.left, prediction.bounding_box.top, prediction.bounding_box.width, prediction.bounding_box.height))\n",
    "                    upload_frame(frame, i)\n",
    "                    save_to_filesystem(frame)\n",
    "                    send_sms()\n",
    "                    time.sleep(time_delay)\n",
    "                else:\n",
    "                    print(\"No object detected\")\n",
    "                    time.sleep(time_delay)\n",
    "            else: \n",
    "                print('Something else was detected')              \n",
    "\n",
    "    cap.release()\n",
    "    print('Released stream')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
